{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ideal_GColab_AM_Train_3D_FastAI.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ohh1dFxEDklD",
        "h-pqPbXCzjOP",
        "mXOqlfwnvOVS",
        "2_h08qlbl7wN",
        "zEGYy4EVl-iu",
        "zjrJZ6LUmlKw",
        "s2iP569WpeKc",
        "cvwdpOPp1VLc",
        "edNsxJUdm5gj",
        "YRFtaJpnL-vu",
        "fdXDS-clMNXq"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zephromonia/MachineLearning-Anderson/blob/devel/Ideal_GColab_AM_Train_3D_FastAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2GhE2tUpeJ2"
      },
      "source": [
        "Image recognition of Anderson states in 3D\r\n",
        "\r\n",
        "This notebook has four main sections:\r\n",
        "\r\n",
        "*   Preliminaries\r\n",
        "  *   Run this at the start to initialize and import many important variables, parameters and libraries. This includes a cell which requires you to authorise the use of a google drive which you will be sent a code to verify the use of.\r\n",
        "  *   The google drive must be formatted as follows:\r\n",
        "      *   MyDrive\r\n",
        "          *    code\r\n",
        "          *    csvdata\r\n",
        "          *    ModelStages\r\n",
        "  *   This format can be tweaked within the code but it ijs likely to cause errors! (see code for more specific guidance on what to call folder and files etc)\r\n",
        "\r\n",
        "*   Functions and main methods\r\n",
        "  *    Cells which contain functions that do all kinds of jobs from loading the data in from folders to optimising hyperparameters\r\n",
        "*   Result Gathering\r\n",
        "  *    Create sub-cells called ***TEST x - description of the test*** then code in a test using the code from the rest of the program. Running only when using the cell to test again ensures that all tests stay in the notebook until it is cleared (as well as many results being saved automatically anyway)\r\n",
        "*   Interpretation\r\n",
        "    *     Confusion matrix for a given epoch that is saved in the google drive\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_902jQztpeJ7"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKzjTI5lpeJ9"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6CB7zxkqOQE"
      },
      "source": [
        "Checking for GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6wqhy4IqNXl"
      },
      "source": [
        "#import tensorflow as tf\r\n",
        "#from tensorflow.python.client import device_lib\r\n",
        "\r\n",
        "#print(tf.test.gpu_device_name())\r\n",
        "#print(device_lib.list_local_devices())\r\n",
        "\r\n",
        "#!cat /proc/meminfo"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhA7iijKy51s"
      },
      "source": [
        "Checking for TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAPEEIDCyxDM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef35482-a1c5-4999-9de4-15a2a529c95b"
      },
      "source": [
        "import os\r\n",
        "assert os.environ['COLAB_TPU_ADDR']\r\n",
        "Path = 'grpc://'+os.environ['COLAB_TPU_ADDR']\r\n",
        "print('TPU Address:', Path)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU Address: grpc://10.72.245.66:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZCnS3VZpeJ-",
        "scrolled": true,
        "outputId": "cfadec15-fe01-48b3-fdee-b48120c0947f"
      },
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "torch.__version__\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#from torchvision import transforms, utils\n",
        "#!pip install torch=='1.7.0'\n",
        "#!pip install torchvision=='0.8.1'\n",
        "import torch\n",
        "import torchvision\n",
        "print(torchvision.__version__)\n",
        "print(torch.__version__)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8.2+cu101\n",
            "1.7.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xWHU6aoazUo"
      },
      "source": [
        "import random\r\n",
        "import math"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L1ptKV1lydV",
        "outputId": "6ea4b37f-3d88-416d-a32d-084e60adcc2f"
      },
      "source": [
        "#!pip install fastai==1.0.61\r\n",
        "import fastai\r\n",
        "from fastai.collab import *\r\n",
        "#!pip install --upgrade fastai\r\n",
        "#!pip uninstall fastai -y\r\n",
        "print(fastai.__version__)\r\n",
        "from fastai.vision import *\r\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx87JLJOWBI-",
        "outputId": "c8fe4568-569f-454c-bbc9-8d07120cce2c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohh1dFxEDklD"
      },
      "source": [
        "###Parameters and variables\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdldBVJ4DnfN"
      },
      "source": [
        "global loaded_training\r\n",
        "global loaded_validation\r\n",
        "global loaded_testing\r\n",
        "global loaded_labels\r\n",
        "loaded_training   = {}\r\n",
        "loaded_validation = {}\r\n",
        "loaded_testing    = {}\r\n",
        "loaded_labels     = {}\r\n",
        "\r\n",
        "d_type_opt = ('training', 'validation')\r\n",
        "\r\n",
        "bs = 40\r\n",
        "epoch_chunk = 3\r\n",
        "t_f_default = 0.7\r\n",
        "v_f_default = 0.2\r\n",
        "global training_fraction\r\n",
        "global validation_fraction\r\n",
        "training_fraction   = t_f_default\r\n",
        "validation_fraction = v_f_default"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VKdGxziD1ej"
      },
      "source": [
        "#Functions and main methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-pqPbXCzjOP"
      },
      "source": [
        "## Generic Functions\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3keJI43Yb6M"
      },
      "source": [
        "def get_files(folder):\r\n",
        "  dir = '/content/drive/MyDrive/csvdata/'+folder+'/'\r\n",
        "  N = 1    #Desired number of input systems per class\r\n",
        "  files = !ls -l {dir}\r\n",
        "  print(files)\r\n",
        "  files.reverse()\r\n",
        "  files.pop()\r\n",
        "  for i in range(len(files)):\r\n",
        "    files[i] = files[i].rsplit(' ',1)[1]\r\n",
        "    print(files[i])\r\n",
        "  return dir,files"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAFNQ9PiXeDr"
      },
      "source": [
        "def to_list(list):\r\n",
        "  py_list = []\r\n",
        "  for i in list:\r\n",
        "    py_list.append(float(i))\r\n",
        "  return py_list"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLhpxg-Yqq-9"
      },
      "source": [
        "def make_3D_structure(file, system_size):\r\n",
        "    col = 0\r\n",
        "    row = 0\r\n",
        "    lay = 0\r\n",
        "    \r\n",
        "    real_3d_with_channel_system = []\r\n",
        "    system = []\r\n",
        "    for i in range(system_size):\r\n",
        "        system.append([])\r\n",
        "        for j in range(system_size):\r\n",
        "            system[-1].append([])\r\n",
        "            for j in range(system_size):\r\n",
        "                system[-1][-1].append([])\r\n",
        "    #print(len(system))\r\n",
        "    #print(len(system[0]))\r\n",
        "    #print(len(system[0][0]))\r\n",
        "    \r\n",
        "    for line in file:\r\n",
        "        if col == system_size:\r\n",
        "            col = 0\r\n",
        "            row += 1\r\n",
        "        if row == system_size:\r\n",
        "            row = 0\r\n",
        "            lay += 1\r\n",
        "        system[col][row][lay] = float(line)\r\n",
        "        #print(float(line))\r\n",
        "        col += 1\r\n",
        "    real_3d_with_channel_system.append(system)\r\n",
        "    return system"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4JNeT1F0Tg0"
      },
      "source": [
        "##Object definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5sO9ug8f111"
      },
      "source": [
        "**Definition of our custom Dataset and label_list class for confusion matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrP-ik6upeKD"
      },
      "source": [
        "class label_list():\n",
        "  def __init__(self, data, label_names):\n",
        "    self.classes = label_names\n",
        "    self.items = data\n",
        "  def get(self,i):\n",
        "    return self.items[i]\n",
        "  def len(self):\n",
        "    return len(self.classes)\n",
        "        "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3j7EqTKRw8b4"
      },
      "source": [
        "class ThreeDDataset(Dataset):\r\n",
        "    def __init__(self, x, y):\r\n",
        "        self.x = []\r\n",
        "        self.y = []\r\n",
        "        class_ind = 0\r\n",
        "        for dis_class in x:\r\n",
        "          for item in dis_class:\r\n",
        "            self.x.append(item)\r\n",
        "            self.y.append(class_ind)\r\n",
        "          class_ind+=1\r\n",
        "        \r\n",
        "        def seed():\r\n",
        "          return 0.1234\r\n",
        "\r\n",
        "        random.shuffle(self.x, seed)\r\n",
        "        random.shuffle(self.y, seed)\r\n",
        "\r\n",
        "        self.y = label_list(self.y, y)\r\n",
        "        #print(\"data: \", len(self.x))\r\n",
        "        #print(\"labels: \", self.y.len(), self.y.items)\r\n",
        "        \r\n",
        "        self.c = len(self.y.classes)\r\n",
        "\r\n",
        "        t = True\r\n",
        "        for dis_class in x:\r\n",
        "          t = (t and len(dis_class)==0)\r\n",
        "        self.is_empty=t\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.x)\r\n",
        "    \r\n",
        "    def __getitem__(self, i):\r\n",
        "        return self.x[i],self.y.get(i)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC9aWxpqf98Z"
      },
      "source": [
        "**Definition of our custom Neural Network Model (not yet in use) **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0HvC38sfz-x"
      },
      "source": [
        "class NNet_old(nn.Module):\r\n",
        "    def __init__(self, num_classes=17):\r\n",
        "        super(NNet, self).__init__()\r\n",
        "        self.layers = nn.Sequential(\r\n",
        "          nn.Conv2d(20,64,5),\r\n",
        "          nn.ReLU(),\r\n",
        "          nn.Conv2d(64,64,5),\r\n",
        "          nn.ReLU()\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.layers(x)\r\n",
        "        return out\r\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhbPLjnXeqYs"
      },
      "source": [
        "class NNet(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(NNet, self).__init__()\r\n",
        "    \r\n",
        "    self.layers = nn.Sequential(\r\n",
        "        # input layer\r\n",
        "        nn.Conv2D(20, 64, ks = (5, 5)),\r\n",
        "            \r\n",
        "        # \"replace negatives with zeroes\"\r\n",
        "        nn.ReLU(),\r\n",
        "            \r\n",
        "        # batch norm\r\n",
        "        nn.BatchNorm2d(num_features=17),\r\n",
        "            \r\n",
        "        # output layer\r\n",
        "        nn.Conv2D(64, 17, ks = (5,5))\r\n",
        "\r\n",
        "    )\r\n",
        "\r\n",
        "    # fastai passes both x_cat and x_cont - we can just ignore x_cat\r\n",
        "    def forward(self, x_cat: Tensor, x_cont: Tensor):\r\n",
        "        # pass x_cont into the network\r\n",
        "        return self.layers(x_cont);"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4VMQWV5gDeh"
      },
      "source": [
        "##Load data from folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pik8R2VDpeKE"
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "def prepare_datasets(folder,d_type,seed, batch_size=1, binning=False,bin_size=1, skip_amount=0, extr=False):\n",
        "  if extr:\n",
        "    binning = False\n",
        "    skip_amount = 0\n",
        "    \n",
        "  folder = folder + d_type + '_' + 'seed' + seed\n",
        "  loaded = False\n",
        "  class_skipping = (skip_amount>0)\n",
        "\n",
        "  loaded = False\n",
        "  if str(folder) in loaded_training.keys():\n",
        "    print(\"Found pre-loaded data \" + folder)\n",
        "    tensors_training   = loaded_training[str(folder)]\n",
        "    tensors_validation = loaded_validation[str(folder)]\n",
        "    tensors_testing    = loaded_testing[str(folder)]\n",
        "    labels             = loaded_labels[str(folder)]\n",
        "    loaded = True\n",
        "\n",
        "  if not loaded:\n",
        "    if d_type=='validation':\n",
        "      training_fraction  =0.1\n",
        "      validation_fraction=0.99\n",
        "    elif d_type=='training':\n",
        "      training_fraction  =t_f_default\n",
        "      validation_fraction=v_f_default\n",
        "\n",
        "    dir = '/content/drive/MyDrive/csvdata/'+folder+'/'\n",
        "    print(\"   Data directory: \", dir)\n",
        "    files = !ls -l {dir}\n",
        "    files.reverse()\n",
        "    files.pop()\n",
        "    for i in range(len(files)):\n",
        "      files[i] = files[i].rsplit(' ',1)[1]\n",
        "    tensors_training   = []  #Each list in this list is a seperate label list\n",
        "    tensors_validation = []  #\n",
        "    tensors_testing    = []  #\n",
        "    global n_labels\n",
        "    n_labels = len(files)    #Defined by the number of data csv files\n",
        "    print(\"Number of classes: \", n_labels)\n",
        "\n",
        "    labels = []\n",
        "    class_val = 0\n",
        "    for class_file in files:\n",
        "      \"\"\"\n",
        "      Add new class to the tensors list, read the systems one by one out \n",
        "      of the data file then add that data to the correct class list in tensors\n",
        "      and add the name of the disorder class to the labels list   \n",
        "      \"\"\"\n",
        "      tensors_training.append([])\n",
        "      tensors_validation.append([])\n",
        "      tensors_testing.append([])\n",
        "      print(class_file)\n",
        "      disorder_class = class_file.rsplit('-',1)[0]\n",
        "      disorder_class = disorder_class.rsplit('W',1)[1]\n",
        "      labels.append(disorder_class)\n",
        "\n",
        "      with open(dir+class_file, 'r') as f:\n",
        "        df = pd.read_csv(f)\n",
        "\n",
        "      for i in range(len(df.transpose())-1):         \n",
        "        \"\"\"\n",
        "        Go through each system and add them to the data\n",
        "        \"\"\"           \n",
        "        ThreeD_array = make_3D_structure(to_list(df[str(i)]),20)\n",
        "        system = torch.tensor(ThreeD_array)\n",
        "\n",
        "        ran_numb = random.random()\n",
        "        if ran_numb >= 1 - training_fraction:\n",
        "          tensors_training[-1].append(system)\n",
        "        \n",
        "        elif ran_numb < validation_fraction:\n",
        "          tensors_validation[-1].append(system)\n",
        "        \n",
        "        else:\n",
        "          tensors_testing[-1].append(system)\n",
        "      class_val+=1\n",
        "    #print(labels)\n",
        "\n",
        "    if str(folder) not in loaded_training.keys():   #Saving the models in a global dictionary for faster loading later\n",
        "      print(\"No pre-existing data \" + folder + \". Saving in dictionary\")\n",
        "      loaded_training[str(folder)]   = tensors_training\n",
        "      loaded_validation[str(folder)] = tensors_validation\n",
        "      loaded_testing[str(folder)]    = tensors_testing\n",
        "      loaded_labels[str(folder)]     = labels\n",
        "\n",
        "\n",
        "    \n",
        "  if class_skipping:    #Skipping every 'skip_amount' classes\n",
        "    new_labels = []\n",
        "    for i in range(len(labels)):\n",
        "      if i*(skip_amount+1) > len(labels)-1:\n",
        "        break\n",
        "      new_labels.append(labels[i*(skip_amount+1)])\n",
        "    labels = new_labels\n",
        "    print(\"After skipping: \", labels)\n",
        "\n",
        "    new_tensors_training = []\n",
        "    for i in range(len(tensors_training)):\n",
        "      if i*(skip_amount+1) > len(tensors_training)-1:\n",
        "        break\n",
        "      new_tensors_training.append(tensors_training[i*(skip_amount+1)])\n",
        "    tensors_training = new_tensors_training\n",
        "\n",
        "    new_tensors_validation = []\n",
        "    for i in range(len(tensors_validation)):\n",
        "      if i*(skip_amount+1) > len(tensors_validation)-1:\n",
        "        break\n",
        "      new_tensors_validation.append(tensors_validation[i*(skip_amount+1)])\n",
        "    tensors_validation = new_tensors_validation\n",
        "\n",
        "    new_tensors_testing = []\n",
        "    for i in range(len(tensors_testing)):\n",
        "      if i*(skip_amount+1) > len(tensors_testing)-1:\n",
        "        break\n",
        "      new_tensors_testing.append(tensors_testing[i*(skip_amount+1)])\n",
        "    tensors_testing = new_tensors_testing\n",
        "\n",
        "  if binning:        #Bin each of the datasets\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"----\")\n",
        "    print(\"   Binning data\")\n",
        "    binned_labels = []\n",
        "    binned_tensors_training = []\n",
        "    binned_tensors_validation = []\n",
        "    binned_tensors_testing = []\n",
        "\n",
        "    binned_labels.append([])   \n",
        "    binned_tensors_training.append([])\n",
        "    binned_tensors_validation.append([])\n",
        "    binned_tensors_testing.append([])\n",
        "\n",
        "    print(len(labels)) \n",
        "    for i in range(len(labels)):       \n",
        "      for label in labels[i]:\n",
        "        binned_labels[-1].append(label)\n",
        "      if (i+1)%bin_size == 0:\n",
        "        binned_labels.append([])\n",
        "    if len(labels)%bin_size != 0:\n",
        "      del binned_labels[-1]\n",
        "    print(\"total bins (la): \", len(binned_labels))\n",
        "    labels = binned_labels\n",
        "    n_labels = len(labels)\n",
        "    print(len(labels))\n",
        "\n",
        "    print(len(tensors_training)) \n",
        "    for i in range(len(tensors_training)):       \n",
        "      for tensor in tensors_training[i]:\n",
        "        binned_tensors_training[-1].append(tensor)\n",
        "      if (i+1)%bin_size == 0:\n",
        "        binned_tensors_training.append([])\n",
        "    if len(tensors_training)%bin_size != 0:\n",
        "      del binned_tensors_training[-1]\n",
        "    print(\"total bins (tr): \", len(binned_tensors_training))\n",
        "    tensors_training = binned_tensors_training\n",
        "\n",
        "    print(len(tensors_validation))\n",
        "    for i in range(len(tensors_validation)):\n",
        "      for tensor in tensors_validation[i]:\n",
        "        binned_tensors_validation[-1].append(tensor)\n",
        "      if (i+1)%bin_size == 0:\n",
        "        binned_tensors_validation.append([])\n",
        "    if len(tensors_validation)%bin_size != 0:\n",
        "      del binned_tensors_validation[-1]\n",
        "    print(\"total bins (va): \", len(binned_tensors_validation))\n",
        "    tensors_validation = binned_tensors_validation\n",
        "\n",
        "    print(len(tensors_testing))\n",
        "    for i in range(len(tensors_testing)):\n",
        "      for tensor in tensors_testing[i]:\n",
        "        binned_tensors_testing[-1].append(tensor)\n",
        "      if (i+1)%bin_size == 0:\n",
        "        binned_tensors_testing.append([])\n",
        "    if len(tensors_testing)%bin_size != 0:\n",
        "      del binned_tensors_testing[-1]\n",
        "    print(\"total bins (te): \", len(binned_tensors_testing))\n",
        "    tensors_testing = binned_tensors_testing\n",
        "    print(\"----\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  if extr:   #Take only the most extreme classes from the data\n",
        "    t_labels = []\n",
        "    t_labels.append(labels[0])\n",
        "    t_labels.append(labels[-1])\n",
        "    labels = t_labels\n",
        "\n",
        "    t_tensors_training = []\n",
        "    t_tensors_training.append(tensors_training[0])\n",
        "    t_tensors_training.append(tensors_training[-1])\n",
        "    tensors_training = t_tensors_training\n",
        "\n",
        "    t_tensors_validation = []\n",
        "    t_tensors_validation.append(tensors_validation[0])\n",
        "    t_tensors_validation.append(tensors_validation[-1])\n",
        "    tensors_validation = t_tensors_validation\n",
        "\n",
        "    t_tensors_testing = []\n",
        "    t_tensors_testing.append(tensors_testing[0])\n",
        "    t_tensors_testing.append(tensors_testing[-1])\n",
        "    tensors_testing = t_tensors_testing\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"----\")\n",
        "    print(\"   Taking only the extreme classes\")\n",
        "    print(\"len labels:     \", len(labels))\n",
        "    print(\"len training:   \", len(tensors_training))\n",
        "    print(\"len validation: \", len(tensors_validation))\n",
        "    print(\"len testing:    \", len(tensors_testing))\n",
        "    print(\"labels =        \", labels)\n",
        "    print(\"----\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "  \n",
        "\n",
        "  loader_training = ThreeDDataset(tensors_training, labels)\n",
        "  loader_validation = ThreeDDataset(tensors_validation, labels)\n",
        "  loader_testing = ThreeDDataset(tensors_testing, labels)\n",
        "  databunch = DataBunch.create(loader_training, loader_validation, loader_testing, bs=bs, num_workers=0)\n",
        "\n",
        "  return databunch"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXOqlfwnvOVS"
      },
      "source": [
        "##Create CnnLearner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPdsytzlpeKF"
      },
      "source": [
        "def create_learner(folder,d_type,seed, show_model=False, batch_size=1, binning=False,bin_size=1, skip_amount=0, opt=optim.Adam, save=True, extr=False):\r\n",
        "  databunch = prepare_datasets(folder, d_type, seed, binning=binning, batch_size=batch_size, bin_size=bin_size, skip_amount=skip_amount, extr=extr)\r\n",
        "  global learn \r\n",
        "  learn = cnn_learner(databunch, models.resnet18, metrics=[accuracy], loss_func=torch.nn.CrossEntropyLoss(), opt_func=opt, pretrained=False)\r\n",
        "  global c_model\r\n",
        "  c_model = 'resnet18-'      #Model\r\n",
        "  metric = 'metric=error_rate-'     #Metric\r\n",
        "  loss = 'loss=CrossEntropyLoss-' #Loss \r\n",
        "  optimizer = 'opt=ADAM-'\r\n",
        "  twod_or_threed = 'CONV2D-'\r\n",
        "  c_model = twod_or_threed+c_model+metric+loss+optimizer\r\n",
        "\r\n",
        "  learn.model[0][0] = nn.Conv2d(20, 64, kernel_size=(3, 3), stride=(1, 1), padding=(0, 0), bias=False)\r\n",
        "  learn.model[1][8] = nn.Linear(in_features=512, out_features=n_labels, bias=True)\r\n",
        "  \r\n",
        "  if show_model:\r\n",
        "    learn.summary()\r\n",
        "\r\n",
        "  if d_type=='training' and save:\r\n",
        "    learn.lr_find(start_lr=1e-4, end_lr=10, num_it=100, stop_div=False)\r\n",
        "    learn.recorder.plot()\r\n",
        "    learn.recorder.plot_lr()\r\n",
        "\r\n",
        "  if save:\r\n",
        "    c_model_t = c_model +'bs='+str(learn.recorder.train_dl.batch_size)+'-ntrain='+folder+d_type+'_'+seed#str(learn.recorder.train_dl.dl.dataset.__len__())\r\n",
        "    !mkdir /content/drive/MyDrive/ModelStages/{c_model_t}\r\n",
        "    learn.save('/content/drive/MyDrive/ModelStages/'\r\n",
        "                                +c_model\r\n",
        "                                +'bs='+str(learn.recorder.train_dl.batch_size)\r\n",
        "                                +'-ntrain='+folder+d_type+'_'+seed #str(learn.recorder.train_dl.dl.dataset.__len__())\r\n",
        "                                +'/epochs-0')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWZ4lLHhPtWM"
      },
      "source": [
        "##Create Custom Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVvnIQNjPrvN"
      },
      "source": [
        "def create_custom_learner(folder,d_type,seed, show_model=False, batch_size=1, binning=False,bin_size=1, skip_amount=0, opt=optim.Adam, save=True):\r\n",
        "  databunch = prepare_datasets(folder, d_type, seed, binning=binning, batch_size=batch_size, bin_size=bin_size, skip_amount=skip_amount)\r\n",
        "  global learn \r\n",
        "  learn = Learner(databunch, NNet, metrics=[accuracy], loss_func=torch.nn.CrossEntropyLoss(), opt_func=opt)\r\n",
        "  global c_model\r\n",
        "  c_model = 'densenet121-'      #Model\r\n",
        "  metric = 'metric=error_rate-'     #Metric\r\n",
        "  loss = 'loss=CrossEntropyLoss-' #Loss \r\n",
        "  optimizer = 'opt=ADAM-'\r\n",
        "  twod_or_threed = 'CONV2D-'\r\n",
        "  c_model = twod_or_threed+c_model+metric+loss+optimizer\r\n",
        "\r\n",
        "  learn.model[0][0] = nn.Conv2d(20, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(0, 0), bias=False)\r\n",
        "  learn.model[1][8] = nn.Linear(in_features=512, out_features=n_labels, bias=True)\r\n",
        "  \r\n",
        "  if show_model:\r\n",
        "    learn.summary()\r\n",
        "\r\n",
        "  if d_type=='training' and save:\r\n",
        "    learn.lr_find(start_lr=1e-4, end_lr=5, num_it=100, stop_div=False)\r\n",
        "    learn.recorder.plot()\r\n",
        "    learn.recorder.plot_lr()\r\n",
        "\r\n",
        "  if save:\r\n",
        "    c_model_t = c_model +'bs='+str(learn.recorder.train_dl.batch_size)+'-ntrain='+folder+d_type+'_'+seed#str(learn.recorder.train_dl.dl.dataset.__len__())\r\n",
        "    !mkdir /content/drive/MyDrive/ModelStages/{c_model_t}\r\n",
        "    learn.save('/content/drive/MyDrive/ModelStages/'\r\n",
        "                                +c_model\r\n",
        "                                +'bs='+str(learn.recorder.train_dl.batch_size)\r\n",
        "                                +'-ntrain='+folder+d_type+'_'+seed #str(learn.recorder.train_dl.dl.dataset.__len__())\r\n",
        "                                +'/epochs-0')"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFdTKB0Gt7Mw"
      },
      "source": [
        "##Baysian Optimisation functions\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWvi__YYuBcI"
      },
      "source": [
        "def fit_with(lr, mom, wd):\r\n",
        "\r\n",
        "  '''\r\n",
        "  You can define your own fit_with() function that takes in the arguments you give to the \r\n",
        "  pbounds object (a dictionary whose keys are the names of the parameters and\r\n",
        "  whose items are tuples containing the bounds for each parameter)\r\n",
        "   and uses them to calculate an 'accuracy' that you then return.\r\n",
        "  optimise_bayesian() then uses this function as a metric to give the optimal \r\n",
        "  values of the parameters you enter (in the ranges you specify in pbounds)\r\n",
        "  '''\r\n",
        "\r\n",
        "  folder = 'raw_data_transfering_4000_'\r\n",
        "  d_type = d_type_opt[0]\r\n",
        "  seed = str(1111)\r\n",
        "  batch_size = 120\r\n",
        "  databunch = prepare_datasets(folder, d_type, batch_size=batch_size, binning=True, bin_size=3, skip_amount=0)\r\n",
        "\r\n",
        "  learn = cnn_learner(databunch, models.densenet121, metrics=[accuracy], loss_func=torch.nn.CrossEntropyLoss(), opt_func=optim.SGD, pretrained=False)\r\n",
        "  learn.model[0][0] = nn.Conv2d(20, 1024, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\r\n",
        "  learn.model[-1][-1] = nn.Linear(in_features=512, out_features=n_labels, bias=True)\r\n",
        "\r\n",
        "  learn.fit_one_cycle(1, max_lr=lr)\r\n",
        "  learn.recorder.plot_losses()\r\n",
        "  acc = float(learn.validate(learn.data.valid_dl)[1])\r\n",
        "\r\n",
        "  print(\"Accuracy:\" + str(acc*100))\r\n",
        "  return acc"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAM6EXefuJLZ"
      },
      "source": [
        "\r\n",
        "def optimise_bayesian(pbounds,fit_with):\r\n",
        "  !pip install bayesian-optimization\r\n",
        "  from bayes_opt import BayesianOptimization\r\n",
        "\r\n",
        "  #pbounds = {'lr': (1e-3, 1e+1), 'mom':(0, 0.99), 'wd':(1e-4, 1)}\r\n",
        "\r\n",
        "  optimizer = BayesianOptimization(\r\n",
        "      f=fit_with,\r\n",
        "      pbounds=pbounds,\r\n",
        "      verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\r\n",
        "      random_state=1,\r\n",
        "  )   \r\n",
        "\r\n",
        "  optimizer.maximize(init_points=5, n_iter=5)\r\n",
        "\r\n",
        "  for i, res in enumerate(optimizer.res):\r\n",
        "      print(\"Iteration {}: \\n\\t{}\".format(i, res))\r\n",
        "\r\n",
        "  print(optimizer.max)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7MgG-ik0bO_"
      },
      "source": [
        "##Training Functions\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shn_kRnrWO7O"
      },
      "source": [
        "def train_max_epochs(epoch_goal, lr=0.01, mom=0, weight_decay=0, epoch_chunk=5, save=True):\r\n",
        "  n_epochs_total = 0\r\n",
        "  #learn.unfreeze()\r\n",
        "  validation_losses = []\r\n",
        "  training_losses = []\r\n",
        "  n_batches = len(learn.recorder.train_dl)\r\n",
        "  while n_epochs_total < epoch_goal:\r\n",
        "    n_epochs_total = load_latest_training(learn, c_model)\r\n",
        "    print(\"Best training so far: \", n_epochs_total)\r\n",
        "\r\n",
        "    learn.fit_one_cycle(epoch_chunk,max_lr=slice(1e-6,1e-4),moms=mom,wd=weight_decay)\r\n",
        "    n_epochs_total += epoch_chunk\r\n",
        "\r\n",
        "    print('/content/drive/MyDrive/ModelStages/'\r\n",
        "      +c_model\r\n",
        "      +'bs='+str(learn.recorder.train_dl.batch_size)\r\n",
        "      +'-ntrain='+folder+d_type#str(learn.recorder.train_dl.dl.dataset.__len__())\r\n",
        "      +'/epochs-'+str(n_epochs_total)\r\n",
        "      +'-max_lr='+str(lr)\r\n",
        "      +'-mom='+str(mom))\r\n",
        "\r\n",
        "    if save:\r\n",
        "      learn.save('/content/drive/MyDrive/ModelStages/'\r\n",
        "      +c_model\r\n",
        "      +'bs='+str(learn.recorder.train_dl.batch_size)\r\n",
        "      +'-ntrain='+folder+d_type+'_'+seed #str(learn.recorder.train_dl.dl.dataset.__len__())\r\n",
        "      +'/epochs-'+str(n_epochs_total)\r\n",
        "      +'-max_lr='+str(lr)\r\n",
        "      +'-mom='+str(mom))\r\n",
        "    \r\n",
        "    for c_val_loss in learn.recorder.val_losses:\r\n",
        "      validation_losses.append(c_val_loss)\r\n",
        "    for i in range(1,epoch_chunk+1):\r\n",
        "      training_losses.append(learn.recorder.losses[i*n_batches-1].item())\r\n",
        "    print(\"losses: \", training_losses)\r\n",
        "    print(\"validation_losses: \", validation_losses)\r\n",
        "  learn.recorder.plot_metrics()\r\n",
        "  learn.recorder.plot_losses()\r\n",
        "  return training_losses, validation_losses"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvxIGCtFU-lO"
      },
      "source": [
        "def train_with_parameters(epochs_max,epoch_chunk=10, batch_size=30, opt=optim.Adam, mom=0, weight_decay=0, lr=0.1):\r\n",
        "  folder = 'raw_data_transfering_4000_'\r\n",
        "  d_type = d_type_opt[0]\r\n",
        "  seed=str(1111)\r\n",
        "\r\n",
        "  create_learner(folder,d_type,seed, batch_size=batch_size, binning=False,bin_size=1, skip_amount=0, opt=opt, save=True)\r\n",
        "  training_losses,validation_losses = train_max_epochs(epochs_max,lr,mom,weight_decay, epoch_chunk,save=False)\r\n",
        "  return training_losses,validation_losses"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHYfjjq3WuiF"
      },
      "source": [
        "def load_latest_training(learn, c_model):\r\n",
        "  dir = '/content/drive/MyDrive/ModelStages/'+c_model+'bs='+str(learn.recorder.train_dl.batch_size)+'-ntrain='+folder+d_type+'_'+seed+'/'\r\n",
        "  training_files = !ls -l {dir}\r\n",
        "  training_files.reverse()\r\n",
        "  training_files.pop()\r\n",
        "\r\n",
        "  for i in range(len(training_files)):\r\n",
        "      training_files[i] = training_files[i].rsplit(' ',1)[1]\r\n",
        "      training_files[i] = training_files[i].rsplit('.',1)[0]\r\n",
        "\r\n",
        "  max_epoch_value = 0\r\n",
        "  max_epoch_ind = -1\r\n",
        "  if len(training_files)!=1:\r\n",
        "    for i in range(len(training_files)-1):\r\n",
        "      training_files[i] = training_files[i].rsplit('\\'',1)[1]\r\n",
        "      if int(training_files[i].rsplit('-',-1)[1].rsplit('-',-1)[0]) > max_epoch_value:\r\n",
        "        max_epoch_value = int(training_files[i].rsplit('-',-1)[1].rsplit('-',-1)[0])\r\n",
        "        max_epoch_ind = i\r\n",
        "  else:\r\n",
        "    max_epoch_ind = 0\r\n",
        "  learn.load(dir+'/'+training_files[max_epoch_ind])\r\n",
        "  \r\n",
        "  \r\n",
        "  return max_epoch_value"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpNV7I5UEN5h"
      },
      "source": [
        "#Result Gathering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYmiN5jtFfQ0"
      },
      "source": [
        "results = {}\r\n",
        "\r\n",
        "def get_losses_bin_skip(bin_size,skip_amount):\r\n",
        "  create_learner(folder,d_type,seed, show_model=False, batch_size=30, binning=True,bin_size=bin_size, skip_amount=skip_amount, save=True)\r\n",
        "  epoch_goal = 10\r\n",
        "  lr = 1\r\n",
        "  mom = 0\r\n",
        "  wd = 1\r\n",
        "  epoch_chunk = 10\r\n",
        "  training_losses,validation_losses = train_max_epochs(epoch_goal,lr,mom,wd,epoch_chunk,save=False)\r\n",
        "  results[str(bin_size)+str(skip_amount)] = [training_losses,validation_losses]\r\n",
        "  print(results)\r\n",
        "get_losses_bin_skip(1,0)\r\n",
        "get_losses_bin_skip(1,1)\r\n",
        "get_losses_bin_skip(1,2)\r\n",
        "get_losses_bin_skip(1,3)\r\n",
        "get_losses_bin_skip(1,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GcerKngql0m"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "print(results['11'][0])\r\n",
        "\r\n",
        "plt.plot(range(len(results['11'][0])), results['11'][0], '-b')\r\n",
        "plt.plot(range(len(results['12'][0])), results['12'][0], '-b')\r\n",
        "plt.plot(range(len(results['13'][0])), results['13'][0], '-b')\r\n",
        "plt.plot(range(len(results['14'][0])), results['14'][0], '-b')\r\n",
        "\r\n",
        "plt.plot(range(len(results['11'][0])), results['11'][1], '-r')\r\n",
        "plt.plot(range(len(results['12'][0])), results['12'][1], '-r')\r\n",
        "plt.plot(range(len(results['13'][0])), results['13'][1], '-r')\r\n",
        "plt.plot(range(len(results['14'][0])), results['14'][1], '-r')\r\n",
        "\r\n",
        "plt.title(\"Train and Valid Losses for different skipping values\")\r\n",
        "plt.xlabel(\"epoch\")\r\n",
        "plt.ylabel(\"loss\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xASsNRWHl0pC"
      },
      "source": [
        "###Local Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUK9-BN0jByT"
      },
      "source": [
        "local_tests = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_h08qlbl7wN"
      },
      "source": [
        "####TEST 1 - Densenet121 4000 systems def hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8BPs_xQhPOsh",
        "outputId": "2f260dd1-feca-4596-aae7-6d8b15c2b5e2"
      },
      "source": [
        "    #TEST 1 - Densenet121 4000 systems def hyperparameters\r\n",
        "folder   = 'raw_data_transfering_4000_'\r\n",
        "d_type   = d_type_opt[0]\r\n",
        "seed     = str(1111)\r\n",
        "sys_size = 20\r\n",
        "\r\n",
        "create_learner(folder, d_type, seed, extr=True)\r\n",
        "local_tests[str(c_model+'-max_e=100-'+'lr=1-'+'epoch_chunk=100-')] = [train_max_epochs(100,lr=1, epoch_chunk=100)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Data directory:  /content/drive/MyDrive/csvdata/raw_data_transfering_4000_training_seed1111/\n",
            "Number of classes:  17\n",
            "W18.0-4000data_seed1111.csv\n",
            "W17.75-4000data_seed1111.csv\n",
            "W17.5-4000data_seed1111.csv\n",
            "W17.25-4000data_seed1111.csv\n",
            "W17.0-4000data_seed1111.csv\n",
            "W16.8-4000data_seed1111.csv\n",
            "W16.7-4000data_seed1111.csv\n",
            "W16.6-4000data_seed1111.csv\n",
            "W16.5-4000data_seed1111.csv\n",
            "W16.4-4000data_seed1111.csv\n",
            "W16.3-4000data_seed1111.csv\n",
            "W16.2-4000data_seed1111.csv\n",
            "W16.0-4000data_seed1111.csv\n",
            "W15.75-4000data_seed1111.csv\n",
            "W15.5-4000data_seed1111.csv\n",
            "W15.25-4000data_seed1111.csv\n",
            "W15.0-4000data_seed1111.csv\n",
            "No pre-existing data raw_data_transfering_4000_training_seed1111. Saving in dictionary\n",
            "['18.0', '17.75', '17.5', '17.25', '17.0', '16.8', '16.7', '16.6', '16.5', '16.4', '16.3', '16.2', '16.0', '15.75', '15.5', '15.25', '15.0']\n",
            "\n",
            "\n",
            "\n",
            "----\n",
            "   Taking only the extreme classes\n",
            "len labels:      2\n",
            "len training:    2\n",
            "len validation:  2\n",
            "len testing:     2\n",
            "labels =         ['18.0', '15.0']\n",
            "----\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/1 00:00<00:00]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='99' class='' max='141' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      70.21% [99/141 00:43<00:18 1.3810]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
            "Best training so far:  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='99' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      99.00% [99/100 1:40:13<01:00]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.825130</td>\n",
              "      <td>2.519966</td>\n",
              "      <td>0.352866</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.441033</td>\n",
              "      <td>2.205426</td>\n",
              "      <td>0.553503</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.216763</td>\n",
              "      <td>1.990909</td>\n",
              "      <td>0.637580</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.058016</td>\n",
              "      <td>1.859247</td>\n",
              "      <td>0.680255</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.931634</td>\n",
              "      <td>1.791680</td>\n",
              "      <td>0.703185</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.823080</td>\n",
              "      <td>1.737212</td>\n",
              "      <td>0.733121</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.817830</td>\n",
              "      <td>1.700183</td>\n",
              "      <td>0.750318</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.720053</td>\n",
              "      <td>1.658275</td>\n",
              "      <td>0.761146</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.704483</td>\n",
              "      <td>1.626734</td>\n",
              "      <td>0.766879</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.650427</td>\n",
              "      <td>1.606878</td>\n",
              "      <td>0.776433</td>\n",
              "      <td>01:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.583958</td>\n",
              "      <td>1.579174</td>\n",
              "      <td>0.777070</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.560532</td>\n",
              "      <td>1.525718</td>\n",
              "      <td>0.778344</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.518017</td>\n",
              "      <td>1.516986</td>\n",
              "      <td>0.776433</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.506296</td>\n",
              "      <td>1.500091</td>\n",
              "      <td>0.780255</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.460048</td>\n",
              "      <td>1.439191</td>\n",
              "      <td>0.782803</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.419031</td>\n",
              "      <td>1.431094</td>\n",
              "      <td>0.784713</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.360778</td>\n",
              "      <td>1.420892</td>\n",
              "      <td>0.784076</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.352684</td>\n",
              "      <td>1.378832</td>\n",
              "      <td>0.785350</td>\n",
              "      <td>01:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.247718</td>\n",
              "      <td>1.325393</td>\n",
              "      <td>0.789809</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.222515</td>\n",
              "      <td>1.295009</td>\n",
              "      <td>0.792357</td>\n",
              "      <td>01:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.203764</td>\n",
              "      <td>1.264250</td>\n",
              "      <td>0.798726</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.173587</td>\n",
              "      <td>1.229644</td>\n",
              "      <td>0.798089</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.161956</td>\n",
              "      <td>1.215566</td>\n",
              "      <td>0.799363</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.099334</td>\n",
              "      <td>1.153136</td>\n",
              "      <td>0.801274</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.078810</td>\n",
              "      <td>1.120896</td>\n",
              "      <td>0.810191</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.026023</td>\n",
              "      <td>1.111646</td>\n",
              "      <td>0.811465</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.998110</td>\n",
              "      <td>1.070493</td>\n",
              "      <td>0.812102</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.943457</td>\n",
              "      <td>1.017282</td>\n",
              "      <td>0.814650</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.905128</td>\n",
              "      <td>1.002726</td>\n",
              "      <td>0.822930</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.858763</td>\n",
              "      <td>0.965128</td>\n",
              "      <td>0.826115</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.796019</td>\n",
              "      <td>0.934058</td>\n",
              "      <td>0.825478</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.768395</td>\n",
              "      <td>0.898790</td>\n",
              "      <td>0.832484</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.773276</td>\n",
              "      <td>0.886050</td>\n",
              "      <td>0.832484</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.726930</td>\n",
              "      <td>0.839457</td>\n",
              "      <td>0.832484</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.713573</td>\n",
              "      <td>0.809507</td>\n",
              "      <td>0.835032</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.677329</td>\n",
              "      <td>0.796951</td>\n",
              "      <td>0.840127</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.643182</td>\n",
              "      <td>0.757221</td>\n",
              "      <td>0.843312</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.612782</td>\n",
              "      <td>0.756109</td>\n",
              "      <td>0.833758</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.594659</td>\n",
              "      <td>0.726264</td>\n",
              "      <td>0.845860</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.560588</td>\n",
              "      <td>0.694499</td>\n",
              "      <td>0.844586</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.534862</td>\n",
              "      <td>0.684463</td>\n",
              "      <td>0.847771</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.513898</td>\n",
              "      <td>0.658249</td>\n",
              "      <td>0.845860</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.497558</td>\n",
              "      <td>0.640012</td>\n",
              "      <td>0.854777</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.475751</td>\n",
              "      <td>0.629277</td>\n",
              "      <td>0.845860</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.468566</td>\n",
              "      <td>0.625729</td>\n",
              "      <td>0.847134</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.469056</td>\n",
              "      <td>0.599541</td>\n",
              "      <td>0.859873</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.428535</td>\n",
              "      <td>0.581483</td>\n",
              "      <td>0.861147</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.412830</td>\n",
              "      <td>0.569743</td>\n",
              "      <td>0.862420</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.422283</td>\n",
              "      <td>0.561600</td>\n",
              "      <td>0.865605</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.384727</td>\n",
              "      <td>0.551922</td>\n",
              "      <td>0.859873</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.379589</td>\n",
              "      <td>0.530221</td>\n",
              "      <td>0.870064</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.379922</td>\n",
              "      <td>0.534086</td>\n",
              "      <td>0.866879</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.339182</td>\n",
              "      <td>0.520961</td>\n",
              "      <td>0.868153</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.362871</td>\n",
              "      <td>0.512180</td>\n",
              "      <td>0.871975</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.342016</td>\n",
              "      <td>0.504414</td>\n",
              "      <td>0.868790</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.321304</td>\n",
              "      <td>0.489173</td>\n",
              "      <td>0.872611</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.329663</td>\n",
              "      <td>0.490780</td>\n",
              "      <td>0.872611</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.318214</td>\n",
              "      <td>0.493850</td>\n",
              "      <td>0.871975</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.296009</td>\n",
              "      <td>0.474104</td>\n",
              "      <td>0.873248</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.301820</td>\n",
              "      <td>0.466875</td>\n",
              "      <td>0.871975</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.298170</td>\n",
              "      <td>0.459739</td>\n",
              "      <td>0.875159</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.292468</td>\n",
              "      <td>0.456518</td>\n",
              "      <td>0.878344</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.293290</td>\n",
              "      <td>0.454075</td>\n",
              "      <td>0.875796</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.263706</td>\n",
              "      <td>0.445669</td>\n",
              "      <td>0.875159</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.271441</td>\n",
              "      <td>0.439150</td>\n",
              "      <td>0.877070</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.261873</td>\n",
              "      <td>0.443897</td>\n",
              "      <td>0.877070</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.253597</td>\n",
              "      <td>0.439788</td>\n",
              "      <td>0.874522</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.260820</td>\n",
              "      <td>0.435021</td>\n",
              "      <td>0.877070</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.246719</td>\n",
              "      <td>0.428556</td>\n",
              "      <td>0.876433</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.242985</td>\n",
              "      <td>0.425660</td>\n",
              "      <td>0.879618</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.245847</td>\n",
              "      <td>0.426133</td>\n",
              "      <td>0.878344</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.246887</td>\n",
              "      <td>0.421003</td>\n",
              "      <td>0.875159</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.238570</td>\n",
              "      <td>0.416956</td>\n",
              "      <td>0.877070</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.226483</td>\n",
              "      <td>0.414693</td>\n",
              "      <td>0.877707</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.215732</td>\n",
              "      <td>0.412214</td>\n",
              "      <td>0.878981</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.224186</td>\n",
              "      <td>0.415613</td>\n",
              "      <td>0.877070</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.227134</td>\n",
              "      <td>0.412706</td>\n",
              "      <td>0.880255</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.228229</td>\n",
              "      <td>0.406596</td>\n",
              "      <td>0.878981</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.218710</td>\n",
              "      <td>0.401999</td>\n",
              "      <td>0.880255</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.226129</td>\n",
              "      <td>0.405425</td>\n",
              "      <td>0.880892</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.217990</td>\n",
              "      <td>0.407533</td>\n",
              "      <td>0.878981</td>\n",
              "      <td>01:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.226034</td>\n",
              "      <td>0.405046</td>\n",
              "      <td>0.877070</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.217298</td>\n",
              "      <td>0.407320</td>\n",
              "      <td>0.876433</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.233525</td>\n",
              "      <td>0.403421</td>\n",
              "      <td>0.879618</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.211391</td>\n",
              "      <td>0.399937</td>\n",
              "      <td>0.880255</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.214676</td>\n",
              "      <td>0.399489</td>\n",
              "      <td>0.877707</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.207766</td>\n",
              "      <td>0.405475</td>\n",
              "      <td>0.877707</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.214625</td>\n",
              "      <td>0.399441</td>\n",
              "      <td>0.878981</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.210295</td>\n",
              "      <td>0.401560</td>\n",
              "      <td>0.878344</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.211428</td>\n",
              "      <td>0.399484</td>\n",
              "      <td>0.880892</td>\n",
              "      <td>01:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.215881</td>\n",
              "      <td>0.398518</td>\n",
              "      <td>0.883439</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.203155</td>\n",
              "      <td>0.392806</td>\n",
              "      <td>0.880255</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.196351</td>\n",
              "      <td>0.395289</td>\n",
              "      <td>0.882166</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.209898</td>\n",
              "      <td>0.395898</td>\n",
              "      <td>0.880255</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.213854</td>\n",
              "      <td>0.401734</td>\n",
              "      <td>0.880255</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.203876</td>\n",
              "      <td>0.398671</td>\n",
              "      <td>0.881529</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.207656</td>\n",
              "      <td>0.396712</td>\n",
              "      <td>0.876433</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.210256</td>\n",
              "      <td>0.395112</td>\n",
              "      <td>0.880892</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.221919</td>\n",
              "      <td>0.399679</td>\n",
              "      <td>0.877707</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='48' class='' max='141' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      34.04% [48/141 00:18<00:35 0.2106]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.825130</td>\n",
              "      <td>2.519966</td>\n",
              "      <td>0.352866</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.441033</td>\n",
              "      <td>2.205426</td>\n",
              "      <td>0.553503</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.216763</td>\n",
              "      <td>1.990909</td>\n",
              "      <td>0.637580</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.058016</td>\n",
              "      <td>1.859247</td>\n",
              "      <td>0.680255</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.931634</td>\n",
              "      <td>1.791680</td>\n",
              "      <td>0.703185</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.823080</td>\n",
              "      <td>1.737212</td>\n",
              "      <td>0.733121</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.817830</td>\n",
              "      <td>1.700183</td>\n",
              "      <td>0.750318</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.720053</td>\n",
              "      <td>1.658275</td>\n",
              "      <td>0.761146</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.704483</td>\n",
              "      <td>1.626734</td>\n",
              "      <td>0.766879</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.650427</td>\n",
              "      <td>1.606878</td>\n",
              "      <td>0.776433</td>\n",
              "      <td>01:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.583958</td>\n",
              "      <td>1.579174</td>\n",
              "      <td>0.777070</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.560532</td>\n",
              "      <td>1.525718</td>\n",
              "      <td>0.778344</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.518017</td>\n",
              "      <td>1.516986</td>\n",
              "      <td>0.776433</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.506296</td>\n",
              "      <td>1.500091</td>\n",
              "      <td>0.780255</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.460048</td>\n",
              "      <td>1.439191</td>\n",
              "      <td>0.782803</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.419031</td>\n",
              "      <td>1.431094</td>\n",
              "      <td>0.784713</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.360778</td>\n",
              "      <td>1.420892</td>\n",
              "      <td>0.784076</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.352684</td>\n",
              "      <td>1.378832</td>\n",
              "      <td>0.785350</td>\n",
              "      <td>01:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.247718</td>\n",
              "      <td>1.325393</td>\n",
              "      <td>0.789809</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.222515</td>\n",
              "      <td>1.295009</td>\n",
              "      <td>0.792357</td>\n",
              "      <td>01:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.203764</td>\n",
              "      <td>1.264250</td>\n",
              "      <td>0.798726</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.173587</td>\n",
              "      <td>1.229644</td>\n",
              "      <td>0.798089</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.161956</td>\n",
              "      <td>1.215566</td>\n",
              "      <td>0.799363</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.099334</td>\n",
              "      <td>1.153136</td>\n",
              "      <td>0.801274</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.078810</td>\n",
              "      <td>1.120896</td>\n",
              "      <td>0.810191</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.026023</td>\n",
              "      <td>1.111646</td>\n",
              "      <td>0.811465</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.998110</td>\n",
              "      <td>1.070493</td>\n",
              "      <td>0.812102</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.943457</td>\n",
              "      <td>1.017282</td>\n",
              "      <td>0.814650</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.905128</td>\n",
              "      <td>1.002726</td>\n",
              "      <td>0.822930</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.858763</td>\n",
              "      <td>0.965128</td>\n",
              "      <td>0.826115</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.796019</td>\n",
              "      <td>0.934058</td>\n",
              "      <td>0.825478</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.768395</td>\n",
              "      <td>0.898790</td>\n",
              "      <td>0.832484</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.773276</td>\n",
              "      <td>0.886050</td>\n",
              "      <td>0.832484</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.726930</td>\n",
              "      <td>0.839457</td>\n",
              "      <td>0.832484</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.713573</td>\n",
              "      <td>0.809507</td>\n",
              "      <td>0.835032</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.677329</td>\n",
              "      <td>0.796951</td>\n",
              "      <td>0.840127</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.643182</td>\n",
              "      <td>0.757221</td>\n",
              "      <td>0.843312</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.612782</td>\n",
              "      <td>0.756109</td>\n",
              "      <td>0.833758</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.594659</td>\n",
              "      <td>0.726264</td>\n",
              "      <td>0.845860</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.560588</td>\n",
              "      <td>0.694499</td>\n",
              "      <td>0.844586</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.534862</td>\n",
              "      <td>0.684463</td>\n",
              "      <td>0.847771</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.513898</td>\n",
              "      <td>0.658249</td>\n",
              "      <td>0.845860</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.497558</td>\n",
              "      <td>0.640012</td>\n",
              "      <td>0.854777</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.475751</td>\n",
              "      <td>0.629277</td>\n",
              "      <td>0.845860</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.468566</td>\n",
              "      <td>0.625729</td>\n",
              "      <td>0.847134</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.469056</td>\n",
              "      <td>0.599541</td>\n",
              "      <td>0.859873</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.428535</td>\n",
              "      <td>0.581483</td>\n",
              "      <td>0.861147</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.412830</td>\n",
              "      <td>0.569743</td>\n",
              "      <td>0.862420</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.422283</td>\n",
              "      <td>0.561600</td>\n",
              "      <td>0.865605</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.384727</td>\n",
              "      <td>0.551922</td>\n",
              "      <td>0.859873</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.379589</td>\n",
              "      <td>0.530221</td>\n",
              "      <td>0.870064</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.379922</td>\n",
              "      <td>0.534086</td>\n",
              "      <td>0.866879</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.339182</td>\n",
              "      <td>0.520961</td>\n",
              "      <td>0.868153</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.362871</td>\n",
              "      <td>0.512180</td>\n",
              "      <td>0.871975</td>\n",
              "      <td>00:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.342016</td>\n",
              "      <td>0.504414</td>\n",
              "      <td>0.868790</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.321304</td>\n",
              "      <td>0.489173</td>\n",
              "      <td>0.872611</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.329663</td>\n",
              "      <td>0.490780</td>\n",
              "      <td>0.872611</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.318214</td>\n",
              "      <td>0.493850</td>\n",
              "      <td>0.871975</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.296009</td>\n",
              "      <td>0.474104</td>\n",
              "      <td>0.873248</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.301820</td>\n",
              "      <td>0.466875</td>\n",
              "      <td>0.871975</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.298170</td>\n",
              "      <td>0.459739</td>\n",
              "      <td>0.875159</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.292468</td>\n",
              "      <td>0.456518</td>\n",
              "      <td>0.878344</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.293290</td>\n",
              "      <td>0.454075</td>\n",
              "      <td>0.875796</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.263706</td>\n",
              "      <td>0.445669</td>\n",
              "      <td>0.875159</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.271441</td>\n",
              "      <td>0.439150</td>\n",
              "      <td>0.877070</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.261873</td>\n",
              "      <td>0.443897</td>\n",
              "      <td>0.877070</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.253597</td>\n",
              "      <td>0.439788</td>\n",
              "      <td>0.874522</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.260820</td>\n",
              "      <td>0.435021</td>\n",
              "      <td>0.877070</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.246719</td>\n",
              "      <td>0.428556</td>\n",
              "      <td>0.876433</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.242985</td>\n",
              "      <td>0.425660</td>\n",
              "      <td>0.879618</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.245847</td>\n",
              "      <td>0.426133</td>\n",
              "      <td>0.878344</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.246887</td>\n",
              "      <td>0.421003</td>\n",
              "      <td>0.875159</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.238570</td>\n",
              "      <td>0.416956</td>\n",
              "      <td>0.877070</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.226483</td>\n",
              "      <td>0.414693</td>\n",
              "      <td>0.877707</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.215732</td>\n",
              "      <td>0.412214</td>\n",
              "      <td>0.878981</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.224186</td>\n",
              "      <td>0.415613</td>\n",
              "      <td>0.877070</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.227134</td>\n",
              "      <td>0.412706</td>\n",
              "      <td>0.880255</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.228229</td>\n",
              "      <td>0.406596</td>\n",
              "      <td>0.878981</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.218710</td>\n",
              "      <td>0.401999</td>\n",
              "      <td>0.880255</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.226129</td>\n",
              "      <td>0.405425</td>\n",
              "      <td>0.880892</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.217990</td>\n",
              "      <td>0.407533</td>\n",
              "      <td>0.878981</td>\n",
              "      <td>01:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.226034</td>\n",
              "      <td>0.405046</td>\n",
              "      <td>0.877070</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.217298</td>\n",
              "      <td>0.407320</td>\n",
              "      <td>0.876433</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.233525</td>\n",
              "      <td>0.403421</td>\n",
              "      <td>0.879618</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.211391</td>\n",
              "      <td>0.399937</td>\n",
              "      <td>0.880255</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.214676</td>\n",
              "      <td>0.399489</td>\n",
              "      <td>0.877707</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.207766</td>\n",
              "      <td>0.405475</td>\n",
              "      <td>0.877707</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.214625</td>\n",
              "      <td>0.399441</td>\n",
              "      <td>0.878981</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.210295</td>\n",
              "      <td>0.401560</td>\n",
              "      <td>0.878344</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.211428</td>\n",
              "      <td>0.399484</td>\n",
              "      <td>0.880892</td>\n",
              "      <td>01:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.215881</td>\n",
              "      <td>0.398518</td>\n",
              "      <td>0.883439</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.203155</td>\n",
              "      <td>0.392806</td>\n",
              "      <td>0.880255</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.196351</td>\n",
              "      <td>0.395289</td>\n",
              "      <td>0.882166</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.209898</td>\n",
              "      <td>0.395898</td>\n",
              "      <td>0.880255</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.213854</td>\n",
              "      <td>0.401734</td>\n",
              "      <td>0.880255</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.203876</td>\n",
              "      <td>0.398671</td>\n",
              "      <td>0.881529</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.207656</td>\n",
              "      <td>0.396712</td>\n",
              "      <td>0.876433</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.210256</td>\n",
              "      <td>0.395112</td>\n",
              "      <td>0.880892</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.221919</td>\n",
              "      <td>0.399679</td>\n",
              "      <td>0.877707</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.201624</td>\n",
              "      <td>0.394030</td>\n",
              "      <td>0.882803</td>\n",
              "      <td>01:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/ModelStages/CONV2D-densenet121-metric=error_rate-loss=CrossEntropyLoss-opt=ADAM-bs=40-ntrain=raw_data_transfering_4000_training/epochs-100-max_lr=1-mom=0\n",
            "losses:  [2.825129508972168, 2.441033124923706, 2.2167630195617676, 2.058015823364258, 1.931633710861206, 1.8230799436569214, 1.8178304433822632, 1.7200530767440796, 1.704483151435852, 1.6504273414611816, 1.5839580297470093, 1.5605323314666748, 1.5180165767669678, 1.5062955617904663, 1.4600483179092407, 1.4190311431884766, 1.3607779741287231, 1.3526841402053833, 1.2477176189422607, 1.2225147485733032, 1.203763723373413, 1.1735872030258179, 1.1619558334350586, 1.0993340015411377, 1.078810214996338, 1.026023268699646, 0.9981096386909485, 0.9434571266174316, 0.9051278233528137, 0.8587629199028015, 0.7960193753242493, 0.7683948278427124, 0.7732759118080139, 0.7269304394721985, 0.7135729789733887, 0.6773287653923035, 0.6431821584701538, 0.6127822995185852, 0.5946587324142456, 0.5605877041816711, 0.5348622798919678, 0.5138978362083435, 0.49755752086639404, 0.4757506251335144, 0.4685657322406769, 0.469056099653244, 0.4285353720188141, 0.4128299951553345, 0.422282874584198, 0.384726881980896, 0.3795892596244812, 0.3799216151237488, 0.3391824960708618, 0.3628709018230438, 0.3420163094997406, 0.32130441069602966, 0.3296632170677185, 0.31821370124816895, 0.29600927233695984, 0.30181992053985596, 0.2981704771518707, 0.2924675941467285, 0.29328975081443787, 0.2637055516242981, 0.2714410126209259, 0.2618730068206787, 0.25359705090522766, 0.26081985235214233, 0.24671851098537445, 0.24298548698425293, 0.2458474040031433, 0.24688710272312164, 0.2385697364807129, 0.22648325562477112, 0.21573178470134735, 0.22418589890003204, 0.22713355720043182, 0.22822929918766022, 0.21870985627174377, 0.22612905502319336, 0.2179904729127884, 0.2260335087776184, 0.2172977477312088, 0.23352523148059845, 0.21139132976531982, 0.21467629075050354, 0.20776619017124176, 0.21462507545948029, 0.21029524505138397, 0.21142785251140594, 0.21588149666786194, 0.20315508544445038, 0.19635070860385895, 0.2098981738090515, 0.21385429799556732, 0.2038756012916565, 0.207655668258667, 0.21025606989860535, 0.2219187468290329, 0.2016238421201706]\n",
            "validation_losses:  [2.5199656, 2.2054265, 1.990909, 1.8592474, 1.79168, 1.7372117, 1.7001834, 1.6582747, 1.6267344, 1.6068778, 1.579174, 1.5257181, 1.516986, 1.5000906, 1.439191, 1.4310936, 1.4208919, 1.3788322, 1.3253931, 1.2950087, 1.2642502, 1.2296437, 1.2155656, 1.1531363, 1.1208956, 1.1116457, 1.0704935, 1.0172818, 1.0027257, 0.96512806, 0.93405783, 0.8987896, 0.88604957, 0.8394567, 0.8095074, 0.7969506, 0.757221, 0.75610864, 0.72626376, 0.6944991, 0.68446255, 0.6582489, 0.6400124, 0.6292768, 0.62572885, 0.5995415, 0.5814833, 0.569743, 0.5616004, 0.5519217, 0.5302205, 0.534086, 0.52096117, 0.51217973, 0.5044137, 0.48917267, 0.49078026, 0.49384975, 0.47410363, 0.46687475, 0.4597388, 0.45651802, 0.45407513, 0.4456686, 0.43915027, 0.44389737, 0.439788, 0.43502143, 0.42855644, 0.4256598, 0.42613316, 0.4210025, 0.41695583, 0.41469282, 0.4122141, 0.41561288, 0.41270608, 0.4065959, 0.40199912, 0.40542513, 0.40753254, 0.4050458, 0.40732017, 0.403421, 0.39993656, 0.39948872, 0.4054749, 0.39944088, 0.4015604, 0.39948395, 0.3985182, 0.39280614, 0.3952888, 0.39589813, 0.40173426, 0.3986712, 0.3967118, 0.39511237, 0.399679, 0.39402968]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([2.825129508972168,\n",
              "  2.441033124923706,\n",
              "  2.2167630195617676,\n",
              "  2.058015823364258,\n",
              "  1.931633710861206,\n",
              "  1.8230799436569214,\n",
              "  1.8178304433822632,\n",
              "  1.7200530767440796,\n",
              "  1.704483151435852,\n",
              "  1.6504273414611816,\n",
              "  1.5839580297470093,\n",
              "  1.5605323314666748,\n",
              "  1.5180165767669678,\n",
              "  1.5062955617904663,\n",
              "  1.4600483179092407,\n",
              "  1.4190311431884766,\n",
              "  1.3607779741287231,\n",
              "  1.3526841402053833,\n",
              "  1.2477176189422607,\n",
              "  1.2225147485733032,\n",
              "  1.203763723373413,\n",
              "  1.1735872030258179,\n",
              "  1.1619558334350586,\n",
              "  1.0993340015411377,\n",
              "  1.078810214996338,\n",
              "  1.026023268699646,\n",
              "  0.9981096386909485,\n",
              "  0.9434571266174316,\n",
              "  0.9051278233528137,\n",
              "  0.8587629199028015,\n",
              "  0.7960193753242493,\n",
              "  0.7683948278427124,\n",
              "  0.7732759118080139,\n",
              "  0.7269304394721985,\n",
              "  0.7135729789733887,\n",
              "  0.6773287653923035,\n",
              "  0.6431821584701538,\n",
              "  0.6127822995185852,\n",
              "  0.5946587324142456,\n",
              "  0.5605877041816711,\n",
              "  0.5348622798919678,\n",
              "  0.5138978362083435,\n",
              "  0.49755752086639404,\n",
              "  0.4757506251335144,\n",
              "  0.4685657322406769,\n",
              "  0.469056099653244,\n",
              "  0.4285353720188141,\n",
              "  0.4128299951553345,\n",
              "  0.422282874584198,\n",
              "  0.384726881980896,\n",
              "  0.3795892596244812,\n",
              "  0.3799216151237488,\n",
              "  0.3391824960708618,\n",
              "  0.3628709018230438,\n",
              "  0.3420163094997406,\n",
              "  0.32130441069602966,\n",
              "  0.3296632170677185,\n",
              "  0.31821370124816895,\n",
              "  0.29600927233695984,\n",
              "  0.30181992053985596,\n",
              "  0.2981704771518707,\n",
              "  0.2924675941467285,\n",
              "  0.29328975081443787,\n",
              "  0.2637055516242981,\n",
              "  0.2714410126209259,\n",
              "  0.2618730068206787,\n",
              "  0.25359705090522766,\n",
              "  0.26081985235214233,\n",
              "  0.24671851098537445,\n",
              "  0.24298548698425293,\n",
              "  0.2458474040031433,\n",
              "  0.24688710272312164,\n",
              "  0.2385697364807129,\n",
              "  0.22648325562477112,\n",
              "  0.21573178470134735,\n",
              "  0.22418589890003204,\n",
              "  0.22713355720043182,\n",
              "  0.22822929918766022,\n",
              "  0.21870985627174377,\n",
              "  0.22612905502319336,\n",
              "  0.2179904729127884,\n",
              "  0.2260335087776184,\n",
              "  0.2172977477312088,\n",
              "  0.23352523148059845,\n",
              "  0.21139132976531982,\n",
              "  0.21467629075050354,\n",
              "  0.20776619017124176,\n",
              "  0.21462507545948029,\n",
              "  0.21029524505138397,\n",
              "  0.21142785251140594,\n",
              "  0.21588149666786194,\n",
              "  0.20315508544445038,\n",
              "  0.19635070860385895,\n",
              "  0.2098981738090515,\n",
              "  0.21385429799556732,\n",
              "  0.2038756012916565,\n",
              "  0.207655668258667,\n",
              "  0.21025606989860535,\n",
              "  0.2219187468290329,\n",
              "  0.2016238421201706],\n",
              " [2.5199656,\n",
              "  2.2054265,\n",
              "  1.990909,\n",
              "  1.8592474,\n",
              "  1.79168,\n",
              "  1.7372117,\n",
              "  1.7001834,\n",
              "  1.6582747,\n",
              "  1.6267344,\n",
              "  1.6068778,\n",
              "  1.579174,\n",
              "  1.5257181,\n",
              "  1.516986,\n",
              "  1.5000906,\n",
              "  1.439191,\n",
              "  1.4310936,\n",
              "  1.4208919,\n",
              "  1.3788322,\n",
              "  1.3253931,\n",
              "  1.2950087,\n",
              "  1.2642502,\n",
              "  1.2296437,\n",
              "  1.2155656,\n",
              "  1.1531363,\n",
              "  1.1208956,\n",
              "  1.1116457,\n",
              "  1.0704935,\n",
              "  1.0172818,\n",
              "  1.0027257,\n",
              "  0.96512806,\n",
              "  0.93405783,\n",
              "  0.8987896,\n",
              "  0.88604957,\n",
              "  0.8394567,\n",
              "  0.8095074,\n",
              "  0.7969506,\n",
              "  0.757221,\n",
              "  0.75610864,\n",
              "  0.72626376,\n",
              "  0.6944991,\n",
              "  0.68446255,\n",
              "  0.6582489,\n",
              "  0.6400124,\n",
              "  0.6292768,\n",
              "  0.62572885,\n",
              "  0.5995415,\n",
              "  0.5814833,\n",
              "  0.569743,\n",
              "  0.5616004,\n",
              "  0.5519217,\n",
              "  0.5302205,\n",
              "  0.534086,\n",
              "  0.52096117,\n",
              "  0.51217973,\n",
              "  0.5044137,\n",
              "  0.48917267,\n",
              "  0.49078026,\n",
              "  0.49384975,\n",
              "  0.47410363,\n",
              "  0.46687475,\n",
              "  0.4597388,\n",
              "  0.45651802,\n",
              "  0.45407513,\n",
              "  0.4456686,\n",
              "  0.43915027,\n",
              "  0.44389737,\n",
              "  0.439788,\n",
              "  0.43502143,\n",
              "  0.42855644,\n",
              "  0.4256598,\n",
              "  0.42613316,\n",
              "  0.4210025,\n",
              "  0.41695583,\n",
              "  0.41469282,\n",
              "  0.4122141,\n",
              "  0.41561288,\n",
              "  0.41270608,\n",
              "  0.4065959,\n",
              "  0.40199912,\n",
              "  0.40542513,\n",
              "  0.40753254,\n",
              "  0.4050458,\n",
              "  0.40732017,\n",
              "  0.403421,\n",
              "  0.39993656,\n",
              "  0.39948872,\n",
              "  0.4054749,\n",
              "  0.39944088,\n",
              "  0.4015604,\n",
              "  0.39948395,\n",
              "  0.3985182,\n",
              "  0.39280614,\n",
              "  0.3952888,\n",
              "  0.39589813,\n",
              "  0.40173426,\n",
              "  0.3986712,\n",
              "  0.3967118,\n",
              "  0.39511237,\n",
              "  0.399679,\n",
              "  0.39402968])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAeuklEQVR4nO3deXgc9Z3n8fdXp2XJt2XwfeEDcxqbYK6EgSTDlZBk2Q0bYENgQnYnB0OYJ0sy5JnJM9nd5EkmQzazT7IOhJAskIMQwhBCSAiGcBkb2/i28X1JlizLOi2ppf7uH10NbcW22raqq7v683qefrqrulT1LRd8VPrVr35l7o6IiMRPSdQFiIhIOBTwIiIxpYAXEYkpBbyISEwp4EVEYqos6gIyjR071qdNmxZ1GSIiBePNN9884O61R/surwJ+2rRpLF++POoyREQKhpntPNZ3aqIREYkpBbyISEwp4EVEYkoBLyISUwp4EZGYUsCLiMSUAl5EJKYU8CIiEfrD+v384MWthDF0uwJeRCRCv129j5++thMzG/R1K+BFRCK0vamT6WOrQ1m3Al5EJEI7mzqYOmZoKOtWwIuIRORQZw+HOhM6gxcRiZvtBzoAmDpGAS8iEis7mzoBmD5WTTQiIrGy/UAHZjBplAJeRCRWdjR1MGFEFUPKS0NZvwJeRCQiO0LsIgkKeBGRyOw4EF4XSVDAi4hE4lBnDy2Hw+siCQp4EZFIhN1FEhTwIiKR2NGUCviwukiCAl5EJBI7DnRiBpNHK+BFRGIl3UWysiycLpKggBcRiUTYXSRBAS8iEomwu0hCyAFvZneb2TozW2tmj5nZkDC3JyJSCJo7wu8iCSEGvJlNBL4ALHT3s4FS4KawticiUijSPWimhdhFEsJvoikDqsysDBgK7At5eyIiee+dgA+xiySEGPDuvhf4NrALqANa3P25/suZ2Z1mttzMljc2NoZVjohI3shFF0kIt4lmFHADMB2YAFSb2S39l3P3xe6+0N0X1tbWhlWOiEjeyEUXSQi3ieb9wHZ3b3T3BPAEcEmI2xMRKQhbGtqZOa4m9O2EGfC7gEVmNtTMDLgK2BDi9kRE8l4y6WxtbOeM2gIOeHdfCjwOrADWBNtaHNb2REQKwd5Dh+lKJDkjB2fwZWGu3N3/EfjHMLchIlJItjS2A+Qk4HUnq4hIDm1tUMCLiMTSloZ2RldXMLq6IvRtKeBFRHJoS0NuLrCCAl5EJGfcnS2NuekiCQp4EZGcaero4VBnIift76CAFxHJmS3BBdZZCngRkXjZksMeNKCAFxHJmS0N7VRXlDJ+RG4ejaGAFxHJka3BBdbU6C3hU8CLiORILrtIggJeRCQn2roS1LV05ayLJCjgRURyYmtj6ilOubrACgp4EZGcyHUPGlDAi4jkxJaGdspLjakhP6YvkwJeRCQHtjS0M21MNWWluYtdBbyISA5s3t/G7NOG5XSbCngRkZC1d/ey62AnZ45XwIuIxMqm+jYA5p4+PKfbVcCLiIRsY30rAHN1Bi8iEi8b6loZVlnGxJFVOd2uAl5EJGQb69qYO35YzsagSVPAi4iEyN3ZWN/GmeNz2/4OCngRkVDtaT5Me3dvzi+wggJeRCRUG9M9aHJ8gRUU8CIiodpYl+pBMyfHNzmBAl5EJFQb69uYOmYo1ZVlOd+2Al5EJEQb6luZe3ruz95BAS8iEprDPX3sONARyQVWUMCLiIRm8/42kk7Ox6BJU8CLiITknSEKdAYvIhIvG+raqCovZUoOH/KRSQEvIhKSjfWtzDl9GCUluR2iIE0BLyISgmTSWbevlXkTommeAQW8iEgodh7spK2rl3MnjoisBgW8iEgI1uxtAeCcSQp4EZFYWbPnEBVlJTl/DmumUAPezEaa2eNmttHMNpjZxWFuT0QkX6ze08KZ44dTXhrdeXTYW/4u8Ky7zwXOAzaEvD0RkcilL7BG2f4OENroN2Y2AngvcBuAu/cAPWFtT0QkX2xv6qC9uzfS9ncI9wx+OtAIPGRmK83sATOr7r+Qmd1pZsvNbHljY2OI5YiI5MaaPcEF1ojP4MMM+DLgAuD77j4f6ADu7b+Quy9294XuvrC2tjbEckREcmP1nhYqy0qYNa4m0joGDHgzm21mz5vZ2mD6XDO7L4t17wH2uPvSYPpxUoEvIhJra/e2cNaE4ZRFeIEVsjuD/yHwZSAB4O6rgZsG+iF3rwd2m9mcYNZVwPqTrFNEpCD0JZ21+1oib56B7C6yDnX3N8yOGEuhN8v1fx54xMwqgG3Ap06wPhGRgrKtsZ3Onj7OmTQy6lKyCvgDZjYTcAAzuxGoy2bl7r4KWHjy5YmIFJbVwQXWcyPuQQPZBfxngcXAXDPbC2wHbg61KhGRArVmbwtV5aXMrI32AitkF/Du7u8PujiWuHubmU0PuzARkUK0JrjAWhrREMGZsrnI+isAd+9w97Zg3uPhlSQiUpgSfUnW7m3h3Dxof4fjnMGb2VzgLGCEmX0s46vhwJCwCxMRKTTr9rXS3ZtkwdRRUZcCHL+JZg5wPTAS+FDG/Dbg02EWJSJSiN7c2QzABVPz/Aze3X8D/MbMLnb313JYk4hIQVqxs5mJI6sYP6Iq6lKA7C6yrjSzz5Jqrnmnacbdbw+tKhGRAuPuLN95kPdMHxN1Ke/I5iLrT4HTgb8GXgQmkWqmERGRwL6WLva3drNgSn40z0B2AX+Gu38V6HD3h4HrgIvCLUtEpLCk298XTB0dcSXvyibgE8H7ITM7GxgBjAuvJBGRwrNiZzNV5aXMHR/dI/r6y6YNfrGZjQLuA54CaoCvhlqViEiBeXNnM+dNHhHpI/r6G7ASd3/A3Zvd/SV3n+Hu44Df5aA2EZGC0NnTy/q61rzp/5523IA3s4vN7EYzGxdMn2tmjwKv5KQ6EZEC8NbuFvqSXjgBb2bfAn4E/Afgt2b2deA5YCkwKzfliYjkvxW7UhdY50/Or4A/Xhv8dcB8d+8K2uB3A2e7+46cVCYiUiDe3NnMzNpqRlVXRF3KEY7XRNPl7l0A7t4MvK1wFxE5UjLprNjVzAVT8uvsHY5/Bj/DzJ7KmJ6eOe3uHw6vLBGRwrCxvo1DnQkumpE/d7CmHS/gb+g3/S9hFiIiUohe29YEwMUzCyjg3f3FXBYiIlKIXtvaxJTRQ5k4Mj8GGMuUPz3yRUQKTF/SWbq9iYvzsHkGFPAiIidt/b5W2rp6ueQMBbyISKy8tu0AQN6ewQ84Fo2Z/Tvg/Wa3AMuB/5vuSikiUmxe29rEjNpqxg3Pz6eYZnMGvw1oB34YvFpJjQc/O5gWESk6vX1Jlu1oztuzd8huNMlL3P3CjOl/N7Nl7n6hma0LqzARkXy2Zm8L7d29edk9Mi2bM/gaM5uSngg+1wSTPaFUJSKS59L93xcV+Bn8PcDLZrYVMGA68LdmVg08HGZxIiL56rWtTcw5bRhjayqjLuWYBgx4d3/GzGYBc4NZmzIurN4fWmUiInmqpzfJ8h3NfPzCyVGXclzZnMEDLACmBcufZ2a4+09Cq0pEJI8t33GQw4k+Lj1jbNSlHFc23SR/CswEVgF9wWwHFPAiUpSWbG6korSES/L4Aitkdwa/EJjn7v37wouIFKUlmxq4cPooqiuzbQSJRja9aNYCp4ddiIhIIdh36DCb97dzxexxUZcyoGx+/YwF1pvZG0B3eqbGgxeRYrRkUyMAV8ypjbiSgWUT8P8UdhEiIoXihU0NTBxZxRnjagZeOGLZdJPUuPAiIqS6R7665QA3zJ+ImUVdzoCO2QZvZi8H721m1prxajOz1mw3YGalZrbSzJ4ejIJFRKKyfMdBOnr6uGJ2/jfPwPGf6HRZ8D7sFLdxF7ABGH6K6xERidSSzY2UlxqX5Hn/97SsxoMPzsInmNmU9CvLn5sEXAc8cCpFiojkgyWbGrhw2mhq8rx7ZFo2Nzp9HvhHYD+QDGY7cG4W678f+BJwzL8CzOxO4E6AKVOy+r0hIpJze5o72by/nRsXTIq6lKxl82voLmCOuzedyIrN7Hqgwd3fNLMrjrWcuy8GFgMsXLhQN1OJSF56bt1+AD4wr3BuC8qmiWY3qSc4nahLgQ+b2Q7gZ8CVZvb/TmI9IiKRe3ZdPXNPH8b0sdVRl5K1bM7gtwFLzOy3HHmj03eO90Pu/mXgywDBGfzfu/stJ1+qiEg0Gtu6WbbjIF+4clbUpZyQbAJ+V/CqCF4iIkXljxv24w5Xn104zTMwQMCbWSkw291vPpWNuPsSYMmprENEJCrPrq1n6pihzD39VHuN59Zx2+DdvQ+YamY6cxeRotRyOMGrWw9w9VmnF8Tdq5mybYN/xcyeAjrSMwdqgxcRiYMXNjaQ6HP+usCaZyC7gN8avEo4Tn92EZE4enZtPeOGVXL+pJFRl3LCshls7Gu5KEREJN8c7uljyeYG/uOCyZSUFFbzDGR3J2stqbtRzwKGpOe7+5Uh1iUiErnnN+6nK5HkmgJsnoHsbnR6BNgITAe+BuwAloVYk4hIXnhy5V5OHz6Ei2bk97NXjyWbgB/j7g8CCXd/0d1vB3T2LiKxdrCjhyWbGrnh/AmUFmDzDGR3kTURvNeZ2XXAPmB0eCWJiETvt6v30Zt0PjJ/YtSlnLRsAv7rZjYCuAf4Hqlx3e8OtSoRkYj9euVe5p4+jDPHF+6jLLLpRZN+ElML8FfhliMiEr2dTR2s2HWIe6+ZG3Upp2TANngzm21mz5vZ2mD6XDO7L/zSRESi8eTKfZjBh8+bEHUppySbi6w/JDUqZALA3VcDN4VZlIhIVNydJ1ftZdH0MUwYWRV1Oackm4Af6u5v9JvXG0YxIiJRW7X7ENsPdPDRAr64mpZNwB8ws5mkHtOHmd0I1IValYhIRB57YxdDK0q55pzCvLkpUza9aD5L6pF6c81sL7AdOKXhg0VE8lHL4QRPvbWPj86fyLAh5VGXc8oGPIN3923u/n6gFpjr7pcBHw29MhGRHHty5V66Ekk+8Z6pUZcyKLJpogHA3TvcvS2Y/GJI9YiIRMLdeXTpLs6ZOIJzJo2IupxBkXXA91OY9+2KiBzDil3NbNrfxs0XTYm6lEFzsgHvg1qFiEjEHlm6i5rKMj5U4H3fMx3zIquZtXH0IDegsDuHiohkONTZw9Or6/hPCydRXZlN35PCcMw9cXc9vUlEisLPl+2mpzc+F1fTTraJRkQkFnp6kzz0yg4umTmGeRMKd2Cxo1HAi0hR++2afdS3dvHpy2dEXcqgU8CLSNFyd3740nZmjavhfbNroy5n0CngRaRovbq1ifV1rXz68hkF+VDtgSjgRaRoLX5pG2NrKrlhfny6RmZSwItIUdpU38aLmxu57ZKpVJaVRl1OKBTwIlKU/u2FLQytKOXmi+LVNTKTAl5Eis6m+jaeXr2P2y6ZxqjqiqjLCY0CXkSKznef30x1RVksu0ZmUsCLSFHZUNfKM2vquf3SeJ+9gwJeRIrM/X/czLAhZdxxWbzP3kEBLyJFZO3eFn6/bj9/c9kMRgwt/Cc2DUQBLyJFwd35X7/bwMih5XzqsmlRl5MTCngRKQrPb2jglS1N3P3+2QyPwfNWsxFawJvZZDN7wczWm9k6M7srrG2JiBxPT2+S//HMBmbWVvOJGD2xaSBhjmzfC9zj7ivMbBjwppn9wd3Xh7hNEZG/8JPXdrD9QAcPfepCykuLp+EitD119zp3XxF8bgM2ABPD2p6IyNEc7Ojhu8+/zftm1/JXc8ZFXU5O5eRXmZlNA+YDS3OxPRGRtG/9fiOdPX3cd92ZUZeSc6EHvJnVAL8C/s7dW4/y/Z1mttzMljc2NoZdjogUkde3NfHYG7u5/dJpzDqt+J5CGmrAm1k5qXB/xN2fONoy7r7Y3Re6+8La2vgNuC8i0ehK9PGVJ9YweXQVX/zAnKjLiURoF1nNzIAHgQ3u/p2wtiMicjT/9qctbDvQwU/veA9VFfEcDnggYZ7BXwrcClxpZquC17Uhbk9EBICN9a384MWtfOyCiVw+q3hbBkI7g3f3l4H4PQNLRPJad28f9/ziLYZXlXPfdfOiLidSYfaDFxHJuX95bjPr9rWy+NYFjI75aJEDKZ4e/yISey+/fYDFL23jlkVT+OBZp0ddTuQU8CISCwc7evjiL1Yxa1wN/3BtcTfNpKmJRkQKXjLp3POLVRzqTPDw7cXba6Y/ncGLSMH71z9u5oVNjXz1Q/M4c/zwqMvJGwp4ESlov1tTx/f+tIWbLpzMLUU0UmQ2FPAiUrA21rdyzy/fYv6UkXzthrNI3V8paQp4ESlIDa1d/M3Dy6muLOMHtyygskzt7v0p4EWk4LR2JfjkQ8s42NHDg59cyGnDh0RdUl5SwItIQelK9HHnT5bz9v42fnDLAs6dNDLqkvKWukmKSMFI9CW5++ereH3bQe7/+Pm8d3bxjjOTDZ3Bi0hBSPQl+cJjK/nd2nq+ev08PjJfD4gbiM7gRSTv9fQm+dyjK3hu/X7uu+5M7rhsetQlFQQFvIjkta5EH597dAV/3NDAP31oHrddqnDPlgJeRPLWwY4e7nh4Gat2H+KfP3I2ty6aGnVJBUUBLyJ5aWdTB7c9tIx9hw7z/Zsv4Oqzx0ddUsFRwItI3nl16wE+/+hKku48+umLWDB1dNQlFSQFvIjkDXdn8Uvb+OazG5lRW8PiWxcwo7Ym6rIKlgJeRPJCS2eCe59Yze/W1nPdOeP55o3nUlOpiDoV+tcTkci9uuUA9/zyLRrbuvnKtXP59OUzNHDYIFDAi0hkDvf08e3nNvHgy9uZUVvNE397iYYeGEQKeBGJxJJNDXz1N2vZffAwty6ayleuPVNPYhpkCngRyal9hw7zP5/ZwNOr65hZW83P7lzEohljoi4rlhTwIpITbV0JfvDiVh7483Yc+OIHZvOZ983QOO4hUsCLSKg6e3p5dOkuvr9kK00dPXx0/kTu+eBsJo0aGnVpsaeAF5FQtHf38sjrO1n80jaaOnq49Iwx/Per5+oiag4p4EVkUO0+2MmPX93BL5btpq27l8tnjeWuq2axcJruRs01BbyInLLeviRLNjXys2W7+NPGBsyM684Zz+2XTef8yTpjj4oCXkROiruzoa6Np97ax69X7mF/azdjayr5zPtmcuuiqUwYWRV1iUVPAS8iWXN31te18ty6/Tyzpo63G9opLTHeO2ssX/vwFK46cxzlpXpQXL5QwIvIcXX29PLa1iZe2tzI8xsb2NN8GDO4cOpovv6Rs7n2nPGMrq6Iukw5CgW8iByhK9HHyl2HeH1bE0u3N7Fi5yF6+pIMKS/hkplj+fyVZ3DVmacxtqYy6lJlAAp4kSKWTDrbmzpYu7eFlbsOsXJXM+vrWkn0OSUG8yYM57ZLp/HeWbUsnDaKIeW6KamQKOBFisTBjh62NLSzqb6VjfVtbKpvY0NdKx09fQBUlZdy7qQR3HHZDBZOHcWF00czoqo84qrlVCjgRWIimXQa27vZ03yYPc2d7GxKvXY0dbCtsZ3mzsQ7yw4bUsac04Zx44JJnDVxBOdMHMGscTWU6QJprCjgRfJcMukcOpygqb2bxvZuDrT30NDaRUNbN/tbu6hr6aI+ePX0JY/42fEjhjBl9FCuOWc8M2trmFFbzezThjFhxBCNt14EQg14M7sa+C5QCjzg7t8Ic3si+ainN8nhnj46enrp6O6lvbuXju4+2rsTtHX10tbVS2tXgtbDvbQcTgSvHpo7EzR39NDc2UPS/3K9FWUljBtWyYQRVZw/eSTjzx7CpFFVTBo1lImjqpgyeqjazItcaAFvZqXA/wE+AOwBlpnZU+6+PqxtSjy4O0nPeMdxh76kkwzmJTM+uzt97qnvk7z7OXh/5xVM9/YF78kkvX1Ob8bnRF8yNd2XpCeYTvQmSfQl6e5L0tP77qu7N0l3bx9diXffuxJ9dCX6OJzo43BP6j3Rd5R0PoqayjKGDyljxNAKRlaVM2tcDaOrKxhdXcGooRWMHVbJ2JoKamsqGTdsCMOrynQWLscV5hn8e4At7r4NwMx+BtwADHrAf+h7L9OVSF0oyu5/pVPjfmJbyWrp4yyU+dWxtn3kMpnz/ejzj7Iadz/qetIBm7md1LS/89n7rcM9Y33B9/0D2zM/A0n3o9aVLyrKSqgsLaGiLPUaUl5KZfpzWSnDq8oZN6ySqopSqspLGVJeSlVFKUOD95rKMoZWllFTWUpNZTk1lWUMG1L2zrvav2WwhRnwE4HdGdN7gIv6L2RmdwJ3AkyZMuWkNjSztvqItkcjB2c1J7iJbBY/3tmYHbFcNsvYUednTmT+O6UXt37rTy9jljk/Y17Gz1u/+eka0t+VWHo99s689LIl6Xlm79RQmjGvxFI/X2JGScnRP5cG06UlwXTJu/PKSoLpEqOspISy0tQ6y0vfnS4rMcpLSygvTU2nPhsVpSWUlpjOlqXgRH6R1d0XA4sBFi5ceFLnb/ffNH9QaxIRiYMw/ybcC0zOmJ4UzBMRkRwIM+CXAbPMbLqZVQA3AU+FuD0REckQWhONu/ea2eeA35PqJvkjd18X1vZERORIobbBu/szwDNhbkNERI5O/bJERGJKAS8iElMKeBGRmFLAi4jElJ3obfdhMrNGYOdJ/vhY4MAgllMIinGfoTj3uxj3GYpzv090n6e6e+3RvsirgD8VZrbc3RdGXUcuFeM+Q3HudzHuMxTnfg/mPquJRkQkphTwIiIxFaeAXxx1AREoxn2G4tzvYtxnKM79HrR9jk0bvIiIHClOZ/AiIpJBAS8iElMFH/BmdrWZbTKzLWZ2b9T1hMXMJpvZC2a23szWmdldwfzRZvYHM3s7eB8Vda2DzcxKzWylmT0dTE83s6XBMf95MBx1rJjZSDN73Mw2mtkGM7s47sfazO4O/ttea2aPmdmQOB5rM/uRmTWY2dqMeUc9tpbyv4P9X21mF5zItgo64DMe7H0NMA/4z2Y2L9qqQtML3OPu84BFwGeDfb0XeN7dZwHPB9NxcxewIWP6m8C/uvsZQDNwRyRVheu7wLPuPhc4j9T+x/ZYm9lE4AvAQnc/m9QQ4zcRz2P9Y+DqfvOOdWyvAWYFrzuB75/Ihgo64Ml4sLe79wDpB3vHjrvXufuK4HMbqf/hJ5La34eDxR4GPhJNheEws0nAdcADwbQBVwKPB4vEcZ9HAO8FHgRw9x53P0TMjzWp4curzKwMGArUEcNj7e4vAQf7zT7Wsb0B+ImnvA6MNLPx2W6r0AP+aA/2nhhRLTljZtOA+cBS4DR3rwu+qgdOi6issNwPfAlIP1V9DHDI3XuD6Tge8+lAI/BQ0DT1gJlVE+Nj7e57gW8Du0gFewvwJvE/1mnHOranlHGFHvBFx8xqgF8Bf+furZnfearPa2z6vZrZ9UCDu78ZdS05VgZcAHzf3ecDHfRrjonhsR5F6mx1OjABqOYvmzGKwmAe20IP+KJ6sLeZlZMK90fc/Ylg9v70n2zBe0NU9YXgUuDDZraDVPPblaTapkcGf8ZDPI/5HmCPuy8Nph8nFfhxPtbvB7a7e6O7J4AnSB3/uB/rtGMd21PKuEIP+KJ5sHfQ9vwgsMHdv5Px1VPAJ4PPnwR+k+vawuLuX3b3Se4+jdSx/ZO73wy8ANwYLBarfQZw93pgt5nNCWZdBawnxseaVNPMIjMbGvy3nt7nWB/rDMc6tk8B/yXoTbMIaMloyhmYuxf0C7gW2AxsBf4h6npC3M/LSP3ZthpYFbyuJdUm/TzwNvBHYHTUtYa0/1cATwefZwBvAFuAXwKVUdcXwv6eDywPjveTwKi4H2vga8BGYC3wU6AyjscaeIzUdYYEqb/W7jjWsQWMVE/BrcAaUr2Mst6WhioQEYmpQm+iERGRY1DAi4jElAJeRCSmFPAiIjGlgBcRiSkFvMSGmbUH79PM7BODvO6v9Jt+dTDXLxIGBbzE0TTghAI+427JYzki4N39khOsSSTnFPASR98ALjezVcEY46Vm9i0zWxaMqf0ZADO7wsz+bGZPkbprEjN70szeDMYlvzOY9w1SoxyuMrNHgnnpvxYsWPdaM1tjZh/PWPeSjDHdHwnu0MTMvmGpcf1Xm9m3c/6vI0VjoLMWkUJ0L/D37n49QBDULe5+oZlVAq+Y2XPBshcAZ7v79mD6dnc/aGZVwDIz+5W732tmn3P384+yrY+Ruuv0PGBs8DMvBd/NB84C9gGvAJea2Qbgo8Bcd3czGznoey8S0Bm8FIMPkhrPYxWpIZbHkHqAAsAbGeEO8AUzewt4ndQgT7M4vsuAx9y9z933Ay8CF2ase4+7J0kNLTGN1DC4XcCDZvYxoPOU907kGBTwUgwM+Ly7nx+8prt7+gy+452FzK4gNarhxe5+HrASGHIK2+3O+NwHlHlqbPP3kBoh8nrg2VNYv8hxKeAljtqAYRnTvwf+WzDcMmY2O3iARn8jgGZ37zSzuaQejZiWSP98P38GPh6089eSehLTG8cqLBjPf4S7PwPcTappRyQUaoOXOFoN9AVNLT8mNYb8NGBFcKGzkaM/+u1Z4L8G7eSbSDXTpC0GVpvZCk8NWZz2a+Bi4C1So31+yd3rg18QRzMM+I2ZDSH1l8UXT24XRQam0SRFRGJKTTQiIjGlgBcRiSkFvIhITCngRURiSgEvIhJTCngRkZhSwIuIxNT/Bzi/pLjw3GcqAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEGYy4EVl-iu"
      },
      "source": [
        "####TEST 2 - Resnet18 4000 systems def hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXtqgkSBlys2",
        "outputId": "02c33cfb-0127-46e9-e4a3-f5595dec3163"
      },
      "source": [
        "    #TEST 2 - Resnet18 4000 systems def hyperparameters\r\n",
        "folder   = 'raw_data_transfering_4000_'\r\n",
        "d_type   = d_type_opt[0]\r\n",
        "seed     = str(1111)\r\n",
        "sys_size = 20\r\n",
        "\r\n",
        "create_learner(folder, d_type, seed, extr=True, batch_size=120)\r\n",
        "local_tests[str(c_model+'-max_e=100-'+'lr=1-'+'epoch_chunk=100-')] = [train_max_epochs(100,lr=1, epoch_chunk=100)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Data directory:  /content/drive/MyDrive/csvdata/raw_data_transfering_4000_training_seed1111/\n",
            "Number of classes:  17\n",
            "W18.0-4000data_seed1111.csv\n",
            "W17.75-4000data_seed1111.csv\n",
            "W17.5-4000data_seed1111.csv\n",
            "W17.25-4000data_seed1111.csv\n",
            "W17.0-4000data_seed1111.csv\n",
            "W16.8-4000data_seed1111.csv\n",
            "W16.7-4000data_seed1111.csv\n",
            "W16.6-4000data_seed1111.csv\n",
            "W16.5-4000data_seed1111.csv\n",
            "W16.4-4000data_seed1111.csv\n",
            "W16.3-4000data_seed1111.csv\n",
            "W16.2-4000data_seed1111.csv\n",
            "W16.0-4000data_seed1111.csv\n",
            "W15.75-4000data_seed1111.csv\n",
            "W15.5-4000data_seed1111.csv\n",
            "W15.25-4000data_seed1111.csv\n",
            "W15.0-4000data_seed1111.csv\n",
            "No pre-existing data raw_data_transfering_4000_training_seed1111. Saving in dictionary\n",
            "\n",
            "\n",
            "\n",
            "----\n",
            "   Taking only the extreme classes\n",
            "len labels:      2\n",
            "len training:    2\n",
            "len validation:  2\n",
            "len testing:     2\n",
            "labels =         ['18.0', '15.0']\n",
            "----\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/1 00:00<00:00]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='99' class='' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      71.22% [99/139 01:24<00:34 1.0716]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
            "Best training so far:  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='74' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      74.00% [74/100 2:38:43<55:46]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.123101</td>\n",
              "      <td>2.889730</td>\n",
              "      <td>0.077253</td>\n",
              "      <td>02:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.026970</td>\n",
              "      <td>2.768129</td>\n",
              "      <td>0.136113</td>\n",
              "      <td>02:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.895588</td>\n",
              "      <td>2.635172</td>\n",
              "      <td>0.248314</td>\n",
              "      <td>02:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.760612</td>\n",
              "      <td>2.478085</td>\n",
              "      <td>0.387492</td>\n",
              "      <td>02:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.620448</td>\n",
              "      <td>2.375193</td>\n",
              "      <td>0.468424</td>\n",
              "      <td>02:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.472199</td>\n",
              "      <td>2.217291</td>\n",
              "      <td>0.553648</td>\n",
              "      <td>02:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.300345</td>\n",
              "      <td>2.093724</td>\n",
              "      <td>0.599632</td>\n",
              "      <td>02:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.129650</td>\n",
              "      <td>1.947232</td>\n",
              "      <td>0.651134</td>\n",
              "      <td>02:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.952541</td>\n",
              "      <td>1.825456</td>\n",
              "      <td>0.686695</td>\n",
              "      <td>02:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.818223</td>\n",
              "      <td>1.680391</td>\n",
              "      <td>0.725322</td>\n",
              "      <td>02:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.619092</td>\n",
              "      <td>1.564139</td>\n",
              "      <td>0.763949</td>\n",
              "      <td>02:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.455176</td>\n",
              "      <td>1.451731</td>\n",
              "      <td>0.783568</td>\n",
              "      <td>02:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.257400</td>\n",
              "      <td>1.314742</td>\n",
              "      <td>0.819129</td>\n",
              "      <td>02:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.091421</td>\n",
              "      <td>1.230593</td>\n",
              "      <td>0.826487</td>\n",
              "      <td>02:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.975068</td>\n",
              "      <td>1.124587</td>\n",
              "      <td>0.840589</td>\n",
              "      <td>02:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.801971</td>\n",
              "      <td>1.043115</td>\n",
              "      <td>0.852238</td>\n",
              "      <td>02:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.681030</td>\n",
              "      <td>1.009031</td>\n",
              "      <td>0.854690</td>\n",
              "      <td>02:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.543918</td>\n",
              "      <td>0.896534</td>\n",
              "      <td>0.877376</td>\n",
              "      <td>02:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.431787</td>\n",
              "      <td>0.866663</td>\n",
              "      <td>0.884120</td>\n",
              "      <td>02:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.325727</td>\n",
              "      <td>0.811196</td>\n",
              "      <td>0.886573</td>\n",
              "      <td>02:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.260726</td>\n",
              "      <td>0.788903</td>\n",
              "      <td>0.886573</td>\n",
              "      <td>02:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.210513</td>\n",
              "      <td>0.768427</td>\n",
              "      <td>0.885346</td>\n",
              "      <td>02:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.167785</td>\n",
              "      <td>0.765494</td>\n",
              "      <td>0.886573</td>\n",
              "      <td>02:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.131408</td>\n",
              "      <td>0.757532</td>\n",
              "      <td>0.883507</td>\n",
              "      <td>02:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.110289</td>\n",
              "      <td>0.755960</td>\n",
              "      <td>0.882894</td>\n",
              "      <td>02:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.095372</td>\n",
              "      <td>0.748403</td>\n",
              "      <td>0.882281</td>\n",
              "      <td>02:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.087005</td>\n",
              "      <td>0.755152</td>\n",
              "      <td>0.882281</td>\n",
              "      <td>02:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.068272</td>\n",
              "      <td>0.748756</td>\n",
              "      <td>0.882894</td>\n",
              "      <td>02:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.059845</td>\n",
              "      <td>0.745772</td>\n",
              "      <td>0.882894</td>\n",
              "      <td>02:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.059984</td>\n",
              "      <td>0.750513</td>\n",
              "      <td>0.879828</td>\n",
              "      <td>02:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.058332</td>\n",
              "      <td>0.761274</td>\n",
              "      <td>0.881055</td>\n",
              "      <td>02:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.051140</td>\n",
              "      <td>0.771112</td>\n",
              "      <td>0.879828</td>\n",
              "      <td>02:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.049055</td>\n",
              "      <td>0.764760</td>\n",
              "      <td>0.879828</td>\n",
              "      <td>02:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.034878</td>\n",
              "      <td>0.769747</td>\n",
              "      <td>0.882894</td>\n",
              "      <td>02:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.038703</td>\n",
              "      <td>0.765600</td>\n",
              "      <td>0.884120</td>\n",
              "      <td>02:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.030621</td>\n",
              "      <td>0.749779</td>\n",
              "      <td>0.881668</td>\n",
              "      <td>02:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.031099</td>\n",
              "      <td>0.764192</td>\n",
              "      <td>0.884120</td>\n",
              "      <td>02:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.033689</td>\n",
              "      <td>0.765059</td>\n",
              "      <td>0.882894</td>\n",
              "      <td>02:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.038961</td>\n",
              "      <td>0.811011</td>\n",
              "      <td>0.884733</td>\n",
              "      <td>02:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.029323</td>\n",
              "      <td>0.803847</td>\n",
              "      <td>0.881668</td>\n",
              "      <td>02:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.021481</td>\n",
              "      <td>0.778462</td>\n",
              "      <td>0.881055</td>\n",
              "      <td>02:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.028702</td>\n",
              "      <td>0.770053</td>\n",
              "      <td>0.881668</td>\n",
              "      <td>02:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.023100</td>\n",
              "      <td>0.773258</td>\n",
              "      <td>0.877376</td>\n",
              "      <td>02:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.020480</td>\n",
              "      <td>0.760596</td>\n",
              "      <td>0.881055</td>\n",
              "      <td>02:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.022529</td>\n",
              "      <td>0.772517</td>\n",
              "      <td>0.884120</td>\n",
              "      <td>02:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.016237</td>\n",
              "      <td>0.760576</td>\n",
              "      <td>0.881055</td>\n",
              "      <td>02:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.017882</td>\n",
              "      <td>0.772833</td>\n",
              "      <td>0.880441</td>\n",
              "      <td>02:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.015153</td>\n",
              "      <td>0.788017</td>\n",
              "      <td>0.882281</td>\n",
              "      <td>02:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.016764</td>\n",
              "      <td>0.785556</td>\n",
              "      <td>0.876150</td>\n",
              "      <td>02:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.015271</td>\n",
              "      <td>0.772502</td>\n",
              "      <td>0.882281</td>\n",
              "      <td>02:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.016348</td>\n",
              "      <td>0.757207</td>\n",
              "      <td>0.880441</td>\n",
              "      <td>02:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.012747</td>\n",
              "      <td>0.793573</td>\n",
              "      <td>0.881055</td>\n",
              "      <td>02:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.012664</td>\n",
              "      <td>0.823300</td>\n",
              "      <td>0.885346</td>\n",
              "      <td>02:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.012047</td>\n",
              "      <td>0.790180</td>\n",
              "      <td>0.882281</td>\n",
              "      <td>02:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.012936</td>\n",
              "      <td>0.797215</td>\n",
              "      <td>0.882281</td>\n",
              "      <td>02:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.011764</td>\n",
              "      <td>0.814837</td>\n",
              "      <td>0.881668</td>\n",
              "      <td>02:13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.011200</td>\n",
              "      <td>0.797263</td>\n",
              "      <td>0.877989</td>\n",
              "      <td>02:13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.011805</td>\n",
              "      <td>0.783650</td>\n",
              "      <td>0.881668</td>\n",
              "      <td>02:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.010070</td>\n",
              "      <td>0.789871</td>\n",
              "      <td>0.879828</td>\n",
              "      <td>02:14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.008675</td>\n",
              "      <td>0.808521</td>\n",
              "      <td>0.881055</td>\n",
              "      <td>02:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.008507</td>\n",
              "      <td>0.791986</td>\n",
              "      <td>0.884120</td>\n",
              "      <td>02:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.008443</td>\n",
              "      <td>0.807971</td>\n",
              "      <td>0.877989</td>\n",
              "      <td>02:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.008076</td>\n",
              "      <td>0.825020</td>\n",
              "      <td>0.881055</td>\n",
              "      <td>02:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.008057</td>\n",
              "      <td>0.812420</td>\n",
              "      <td>0.881668</td>\n",
              "      <td>02:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.007714</td>\n",
              "      <td>0.791230</td>\n",
              "      <td>0.882281</td>\n",
              "      <td>02:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.006932</td>\n",
              "      <td>0.807405</td>\n",
              "      <td>0.879828</td>\n",
              "      <td>02:14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.006905</td>\n",
              "      <td>0.817369</td>\n",
              "      <td>0.882894</td>\n",
              "      <td>02:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.010116</td>\n",
              "      <td>0.821788</td>\n",
              "      <td>0.882894</td>\n",
              "      <td>02:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.007727</td>\n",
              "      <td>0.811230</td>\n",
              "      <td>0.881055</td>\n",
              "      <td>02:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.007055</td>\n",
              "      <td>0.797831</td>\n",
              "      <td>0.880441</td>\n",
              "      <td>02:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.008503</td>\n",
              "      <td>0.808543</td>\n",
              "      <td>0.881055</td>\n",
              "      <td>02:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.006947</td>\n",
              "      <td>0.848407</td>\n",
              "      <td>0.882894</td>\n",
              "      <td>02:14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.008304</td>\n",
              "      <td>0.830551</td>\n",
              "      <td>0.878602</td>\n",
              "      <td>02:13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.005936</td>\n",
              "      <td>0.812871</td>\n",
              "      <td>0.879828</td>\n",
              "      <td>02:12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='8' class='' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      5.76% [8/139 00:07<01:54 0.0057]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjrJZ6LUmlKw"
      },
      "source": [
        "####TEST 3 - Densenet121 4000 systems optimised hyperparameters (Bayesian)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ammSAEvXnGXy"
      },
      "source": [
        "    #TEST 3 - Densenet 4000 systems optimised hyperparameters (Bayesian)\r\n",
        "\r\n",
        "from bayes_opt import BayesianOptimization\r\n",
        "\r\n",
        "pbounds = {'lr': (1e-3, 1e+1), 'mom':(0, 0.99), 'wd':(1e-4, 1)}\r\n",
        "\r\n",
        "optimizer = BayesianOptimization(\r\n",
        "    f=fit_with,\r\n",
        "    pbounds=pbounds,\r\n",
        "    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\r\n",
        "    random_state=1,\r\n",
        ")   \r\n",
        "\r\n",
        "optimizer.maximize(init_points=5, n_iter=5)\r\n",
        "\r\n",
        "for i, res in enumerate(optimizer.res):\r\n",
        "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\r\n",
        "\r\n",
        "print(optimizer.max)\r\n",
        "best_lr           =\r\n",
        "best_mom          =\r\n",
        "best_weight_decay =\r\n",
        "\r\n",
        "folder   = 'raw_data_transfering_4000_'\r\n",
        "d_type   = d_type_opt[0]\r\n",
        "seed     = str(1111)\r\n",
        "sys_size = 20\r\n",
        "\r\n",
        "create_learner(folder, d_type, seed, extr=True, batch_size=120)\r\n",
        "local_tests[str(c_model+'-max_e=100-'+'lr=1-'+'epoch_chunk=100-')] = [train_with_parameters(100,lr=best_lr,mom=best_mom, weight_decay=best_weight_decay epoch_chunk=100)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHJiROCaQJkj"
      },
      "source": [
        "####TEST 4 - Create Custom Learner experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHwsLrMkQFIP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "outputId": "ebb1653a-82d5-42f4-e190-5829d27aeb53"
      },
      "source": [
        "folder = 'raw_data_transfering_200_'\r\n",
        "d_type = d_type_opt[0]\r\n",
        "seed=str(1111)\r\n",
        "create_custom_learner(folder, d_type, seed)\r\n",
        "train_max_epochs(1, 0.1, 0.1, 0.1, 1, save = False)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Data directory:  /content/drive/MyDrive/csvdata/raw_data_transfering_200_training_seed1111/\n",
            "Number of classes:  17\n",
            "W18.0-200data_seed1111.csv\n",
            "W17.75-200data_seed1111.csv\n",
            "W17.5-200data_seed1111.csv\n",
            "W17.25-200data_seed1111.csv\n",
            "W17.0-200data_seed1111.csv\n",
            "W16.8-200data_seed1111.csv\n",
            "W16.7-200data_seed1111.csv\n",
            "W16.6-200data_seed1111.csv\n",
            "W16.5-200data_seed1111.csv\n",
            "W16.4-200data_seed1111.csv\n",
            "W16.3-200data_seed1111.csv\n",
            "W16.2-200data_seed1111.csv\n",
            "W16.0-200data_seed1111.csv\n",
            "W15.75-200data_seed1111.csv\n",
            "W15.5-200data_seed1111.csv\n",
            "W15.25-200data_seed1111.csv\n",
            "W15.0-200data_seed1111.csv\n",
            "No pre-existing data raw_data_transfering_200_training_seed1111. Saving in dictionary\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-b33cf445f6ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0md_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_type_opt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcreate_custom_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_max_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-9121ad715938>\u001b[0m in \u001b[0;36mcreate_custom_learner\u001b[0;34m(folder, d_type, seed, show_model, batch_size, binning, bin_size, skip_amount, opt, save)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mdatabunch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbin_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbin_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_amount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_amount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mglobal\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabunch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mglobal\u001b[0m \u001b[0mc_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mc_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'densenet121-'\u001b[0m      \u001b[0;31m#Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, model, opt_func, loss_func, metrics, true_wd, bn_wd, wd, train_bn, path, model_dir, callback_fns, callbacks, layer_groups, add_time, silent)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;34m\"Setup path,metrics, callbacks and ensure model directory exists.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'torch.device' object has no attribute '_apply'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2iP569WpeKc"
      },
      "source": [
        "# Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOXsaUPdpeKd"
      },
      "source": [
        "def show_confs(c_model):\r\n",
        "  dir = '/content/drive/MyDrive/ModelStages/'+c_model\r\n",
        "  epochs = !ls -l {dir}\r\n",
        "  epochs.reverse()\r\n",
        "  epochs.pop()\r\n",
        "\r\n",
        "  max_epoch_value = 0\r\n",
        "  for i in range(len(epochs)):\r\n",
        "    epochs[i] = epochs[i].rsplit(' ',1)[1]\r\n",
        "    epochs[i] = epochs[i].rsplit('.',1)[0]\r\n",
        "  print(epochs)\r\n",
        "\r\n",
        "  for epoch in epochs:\r\n",
        "    learn.load('/content/drive/MyDrive/ModelStages/'+c_model+'/'+epoch);\r\n",
        "    preds,y,losses = learn.get_preds(with_loss=True)\r\n",
        "    interp = ClassificationInterpretation(learn,preds,y,losses)\r\n",
        "    interp.plot_confusion_matrix(figsize=(7,7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynclH65EOwUu"
      },
      "source": [
        "#show_confs(c_model)\r\n",
        "\r\n",
        "#learn.load('/content/drive/MyDrive/ModelStages/'+c_model+'/epochs-10');\r\n",
        "learn.load('/content/drive/MyDrive/ModelStages/'+'CONV2D-resnet18_error_rate_loss=default'+'/epochs-210');\r\n",
        "preds,y,losses = learn.get_preds(with_loss=True)\r\n",
        "interp = ClassificationInterpretation(learn,preds,y,losses)\r\n",
        "interp.plot_confusion_matrix(figsize=(7,7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvwdpOPp1VLc"
      },
      "source": [
        "##General Testing Area"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB7MOj3Z1TS7"
      },
      "source": [
        "folder = 'raw_data_transfering_200_'\r\n",
        "d_type = d_type_opt[0]\r\n",
        "\r\n",
        "create_learner(folder, d_type, save=False)\r\n",
        "\r\n",
        "preds,targs = learn.get_preds()\r\n",
        "print(targs)\r\n",
        "print(preds)\r\n",
        "print(len(preds))\r\n",
        "\r\n",
        "learn.pred_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edNsxJUdm5gj"
      },
      "source": [
        "#Hyperparameter Optimisation Test area"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRFtaJpnL-vu"
      },
      "source": [
        "##Our own grid search algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWk2jSmKME5N"
      },
      "source": [
        "parameter_lims = {}\r\n",
        "parameter_lims['bs']  = [20,30,40]\r\n",
        "parameter_lims['opt'] = [optim.Adam,optim.AdamW]\r\n",
        "parameter_lims['mom'] = [0,0.5,0.9]\r\n",
        "lr = 0.1\r\n",
        "\r\n",
        "for bs_val in parameter_lims['bs']:\r\n",
        "  for opt_val in parameter_lims['opt']:\r\n",
        "    for mom_val in parameter_lims['mom']:\r\n",
        "      train_losses,valid_losses = train_with_parameters(1,bs_val, opt_val, mom_val, 0, lr)\r\n",
        "      print(\"parameters: bs_val=\", bs_val, \" opt=\", opt_val, \" mom=\", mom_val)\r\n",
        "      print(\"train_loss=\", train_losses[-1])\r\n",
        "      print(\"valid_loss=\", valid_losses[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdXDS-clMNXq"
      },
      "source": [
        "##Online grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnOUBnSHm4Nt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d0634b9-4300-435b-9f34-4086307ff090"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from torch import nn\r\n",
        "import numpy as np\r\n",
        "from sklearn.datasets import make_classification\r\n",
        "!pip install --user skorch\r\n",
        "from skorch import NeuralNetClassifier\r\n",
        "\r\n",
        "class MyModule(nn.Module):\r\n",
        "    def __init__(self, num_units=10, nonlin=nn.ReLU()):\r\n",
        "        super(MyModule, self).__init__()\r\n",
        "\r\n",
        "        self.dense0 = nn.Linear(20, num_units)\r\n",
        "        self.nonlin = nonlin\r\n",
        "        self.dropout = nn.Dropout(0.5)\r\n",
        "        self.dense1 = nn.Linear(num_units, num_units)\r\n",
        "        self.output = nn.Linear(num_units, 2)\r\n",
        "        self.softmax = nn.Softmax(dim=-1)\r\n",
        "\r\n",
        "    def forward(self, X, **kwargs):\r\n",
        "        X = self.nonlin(self.dense0(X))\r\n",
        "        X = self.dropout(X)\r\n",
        "        X = self.nonlin(self.dense1(X))\r\n",
        "        X = self.softmax(self.output(X))\r\n",
        "        return X\r\n",
        "\r\n",
        "\r\n",
        "net = NeuralNetClassifier(\r\n",
        "    MyModule,\r\n",
        "    max_epochs=10,\r\n",
        "    lr=0.1,\r\n",
        "    # Shuffle training data on each epoch\r\n",
        "    iterator_train__shuffle=True,\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "net.set_params(train_split=False, verbose=0)\r\n",
        "params = {\r\n",
        "    'lr': [0.01, 0.02],\r\n",
        "    'max_epochs': [10, 20],\r\n",
        "    'module__num_units': [10, 20],\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "X, y = make_classification(1000, 20, n_informative=10, random_state=0)\r\n",
        "X = X.astype(np.float32)\r\n",
        "y = y.astype(np.int64)\r\n",
        "\r\n",
        "gs = GridSearchCV(net, params, refit=False, cv=3, scoring='accuracy', verbose=2)\r\n",
        "gs.fit(X, y)\r\n",
        "print(\"best score: {:.3f}, best params: {}\".format(gs.best_score_, gs.best_params_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting skorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/c7/2f6434f9360c91a4bf14ae85f634758e5dacd3539cca4266a60be9f881ae/skorch-0.9.0-py3-none-any.whl (125kB)\n",
            "\r\u001b[K     |                             | 10kB 13.4MB/s eta 0:00:01\r\u001b[K     |                          | 20kB 18.1MB/s eta 0:00:01\r\u001b[K     |                        | 30kB 10.9MB/s eta 0:00:01\r\u001b[K     |                     | 40kB 8.5MB/s eta 0:00:01\r\u001b[K     |                   | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |                | 61kB 6.0MB/s eta 0:00:01\r\u001b[K     |             | 71kB 6.1MB/s eta 0:00:01\r\u001b[K     |           | 81kB 6.7MB/s eta 0:00:01\r\u001b[K     |        | 92kB 6.2MB/s eta 0:00:01\r\u001b[K     |      | 102kB 6.7MB/s eta 0:00:01\r\u001b[K     |   | 112kB 6.7MB/s eta 0:00:01\r\u001b[K     || 122kB 6.7MB/s eta 0:00:01\r\u001b[K     || 133kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from skorch) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.4.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from skorch) (0.8.8)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->skorch) (1.0.1)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dddd67a0de1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install --user skorch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeuralNetClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMyModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skorch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}